{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43ad20a2-5282-4203-b0a8-cd0dcca29564",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "181790b2-922e-41e8-9236-bdcb37befc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import functools\n",
    "import glob\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import operator\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import pickle\n",
    "import sys\n",
    "# import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202ce867-8bf4-4d82-b7c9-3773e5d743b5",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612f5f2f-f9dd-4c8f-bb50-914d80bddcc4",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65154f5-f3f9-484d-854b-87affdcb72f9",
   "metadata": {},
   "source": [
    "The functions defined in this notebook can be used to preprocess the APRIO data.  The original documents are in PDF format, but they have been converted to JSON files using OCR.  The JSON files represent lists of page objects.  Each page object has a list of lines.  Each line object has a text element.  The second step in the preprocessing is the aggregation of all these text element into a single text file.  These text files serve as input for the preprocessing steps defined in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c223000-ca32-4b0c-bf88-e7fe04804c9f",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cb5333-badb-4531-9593-31703ed5a9f3",
   "metadata": {},
   "source": [
    "The nltk package contains many useful algorithms as well as data for natural language processing in general, and this task in particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c54a6c5e-793d-4560-858c-49e44df3806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/2-papers-text/5524.txt') as file:\n",
    "    article = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200d7af2-ad45-4110-bf60-b5e31db76459",
   "metadata": {},
   "source": [
    "The `sent_tokenize` function will tokenize a text on the level of sentences.  Subsequently, the `word_tokenize` function will tokenize a sentence into individual words as well as punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53cd14fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "# tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c33069d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72da37f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9137db0-c137-4a37-8996-8ee6777a1aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sentences = nltk.sent_tokenize(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "322ed3e1-8991-44dc-8400-6fe4db09acb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d4c940f-6629-4e76-93b2-0fe56bec9764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['el\\nSCIENTIFIC OPINION\\nEFSA Journal\\nADOPTED: 27 November 2018\\ndoi: 10.2903/j.efsa.2019.5524\\nSafety and efficacy of Lactobacillus reuteri NBF-1\\n(DSM 32203) as a feed additive for dogs\\nEFSA Panel on Additives and Products or Substances used in Animal Feed (FEEDAP),\\nVasileios Bampidis, Giovanna Azimonti, Maria de Lourdes Bastos, Henrik Christensen,\\nBirgit Dusemund, Maryline Kouba, Mojca Kos Durjava, Marta Lopez-Alonso,\\nSecundino Lopez Puente, Francesca Marcon, Baltasar Mayo, Alena Pechov a,\\nMariana Petkova, Fernando Ramos, Yolanda Sanz, Roberto Edoardo Villa, Ruud Woutersen,\\nAndrew Chesson, Pier Sandro Cocconcelli, Robert John Wallace, Guido Rychen,\\nRosella Brozzi and Maria Saarela\\nAbstract\\nFollowing a request from the European Commission, the Panel on Additives and Products or\\nSubstances used in Animal Feed (FEEDAP) was asked to deliver a scientific opinion on the safety and\\nefficacy of Lactobacillus reuteri NBF-1 when used in feed for dogs at a minimum dose of\\n6 9 109 colony forming units (CFU) per animal and day.',\n",
       " 'The additive is a preparation of viable cells of\\nL. reuteri DSM 32203.',\n",
       " 'This species is considered by the European Food Safety Authority to be suitable\\nfor the qualified presumption of safety (QPS) approach establishing safety for the target species and\\nthe environment.',\n",
       " 'The active agent fulfils the requirements of the QPS approach to the assessment of\\nsafety.',\n",
       " 'Consequently, in the absence of concerns from other components of the additive,\\nLactobacillus reuteri\\nNBF-1\\nis\\npresumed safe for the target animals and the environment.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a09fdf-83c5-4cd2-8044-115ed81fd783",
   "metadata": {},
   "source": [
    "It is clear that the input document contains sentences that do not make nuch sense.  These are concatenations of headers and footers of each page.  It would be possible to improve te preprocessing so that the text documents that serve as input here would be cleaner, but for this is irrelevant for the preprocessing steps we define here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c3e68776-4bdd-4585-a6f7-65f3adf14fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'an',\n",
       " 'open',\n",
       " 'access',\n",
       " 'article',\n",
       " 'under',\n",
       " 'the',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Creative',\n",
       " 'Commons',\n",
       " 'Attribution-NoDerivs',\n",
       " 'License',\n",
       " ',',\n",
       " 'which',\n",
       " 'permits',\n",
       " 'use',\n",
       " 'and',\n",
       " 'distribution',\n",
       " 'in',\n",
       " 'any',\n",
       " 'medium',\n",
       " ',',\n",
       " 'provided',\n",
       " 'the',\n",
       " 'original',\n",
       " 'work',\n",
       " 'is',\n",
       " 'properly',\n",
       " 'cited',\n",
       " 'and',\n",
       " 'no',\n",
       " 'modifications',\n",
       " 'or',\n",
       " 'adaptations',\n",
       " 'are',\n",
       " 'made',\n",
       " '.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.tokenize.word_tokenize(sentences[17])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c594f0-daf7-4d28-86cb-53864aaddcf1",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28019046-fea2-4917-9bd0-006f0481f271",
   "metadata": {},
   "source": [
    "The first preprocessing task that is required for both TF-IDF and word embeddings is the normalization of the text.  Only words are retained, i.e., only strings that contain alphebetic characters only (using Python's `isalpha` function, hence UTF-8 complient).  The words are converted to lower case, and stemmed using the Snowball stemmer.  The result is a list of lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68c825dd-e604-4991-8010-60293b771261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text, stemmer=None, reverse_dict=None):\n",
    "    if stemmer is None:\n",
    "        stemmer = nltk.stem.snowball.EnglishStemmer()\n",
    "    words = []\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            if word.isalpha():\n",
    "                stem = stemmer.stem(word.lower())\n",
    "                words.append(stem)\n",
    "                if reverse_dict is not None:\n",
    "                    if stem not in reverse_dict:\n",
    "                        reverse_dict[stem] = {word}\n",
    "                    else:\n",
    "                        reverse_dict[stem].add(word)\n",
    "#        words.extend(\n",
    "#            stemmer.stem(word.lower()) for word in nltk.word_tokenize(sentence) if word.isalpha()\n",
    "#        )\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edf4f16f-2265-494f-9fdf-8e5bdd0c2b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_words = normalize(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45fffe50-737a-4b6b-8f4d-15c097075b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3614"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dadf66f5-a692-411e-be85-0e2302842912",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acb3e5ad-df0e-45f5-a489-e98f91124b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_words = normalize(article, reverse_dict=reverse_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e3ca99b-68c7-4671-809b-a5d04ef0985f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'el': {'el'},\n",
       " 'scientif': {'SCIENTIFIC', 'Scientific', 'scientific'},\n",
       " 'opinion': {'OPINION', 'Opinion', 'opinion'},\n",
       " 'efsa': {'EFSA', 'efsa'},\n",
       " 'journal': {'Journal'},\n",
       " 'adopt': {'ADOPTED', 'adopted'},\n",
       " 'novemb': {'November'},\n",
       " 'doi': {'doi'},\n",
       " 'safeti': {'Safety', 'safety'},\n",
       " 'and': {'and'},\n",
       " 'efficaci': {'Efficacy', 'efficacy'},\n",
       " 'of': {'of'},\n",
       " 'lactobacillus': {'Lactobacillus'},\n",
       " 'reuteri': {'reuteri'},\n",
       " 'dsm': {'DSM'},\n",
       " 'as': {'as'},\n",
       " 'a': {'A', 'a'},\n",
       " 'feed': {'FEED', 'Feed', 'feed'},\n",
       " 'addit': {'Additional', 'Additionally', 'Additives', 'additive', 'additives'},\n",
       " 'for': {'For', 'for'},\n",
       " 'dog': {'dog', 'dogs'},\n",
       " 'panel': {'Panel'},\n",
       " 'on': {'on'},\n",
       " 'product': {'Products', 'product', 'production', 'products'},\n",
       " 'or': {'or'},\n",
       " 'substanc': {'Substances', 'substance'},\n",
       " 'use': {'use', 'used', 'using'},\n",
       " 'in': {'In', 'in'},\n",
       " 'anim': {'Animal', 'animal', 'animals'},\n",
       " 'feedap': {'FEEDAP', 'feedap'},\n",
       " 'vasileio': {'Vasileios'},\n",
       " 'bampidi': {'Bampidis'},\n",
       " 'giovanna': {'Giovanna'},\n",
       " 'azimonti': {'Azimonti'},\n",
       " 'maria': {'Maria'},\n",
       " 'de': {'de'},\n",
       " 'lourd': {'Lourdes'},\n",
       " 'basto': {'Bastos'},\n",
       " 'henrik': {'Henrik'},\n",
       " 'christensen': {'Christensen'},\n",
       " 'birgit': {'Birgit'},\n",
       " 'dusemund': {'Dusemund'},\n",
       " 'marylin': {'Maryline'},\n",
       " 'kouba': {'Kouba'},\n",
       " 'mojca': {'Mojca'},\n",
       " 'kos': {'Kos'},\n",
       " 'durjava': {'Durjava'},\n",
       " 'marta': {'Marta'},\n",
       " 'secundino': {'Secundino'},\n",
       " 'lopez': {'Lopez'},\n",
       " 'puent': {'Puente'},\n",
       " 'francesca': {'Francesca'},\n",
       " 'marcon': {'Marcon'},\n",
       " 'baltasar': {'Baltasar'},\n",
       " 'mayo': {'Mayo'},\n",
       " 'alena': {'Alena'},\n",
       " 'pechov': {'Pechov'},\n",
       " 'mariana': {'Mariana'},\n",
       " 'petkova': {'Petkova'},\n",
       " 'fernando': {'Fernando'},\n",
       " 'ramo': {'Ramos'},\n",
       " 'yolanda': {'Yolanda'},\n",
       " 'sanz': {'Sanz'},\n",
       " 'roberto': {'Roberto'},\n",
       " 'edoardo': {'Edoardo'},\n",
       " 'villa': {'Villa'},\n",
       " 'ruud': {'Ruud'},\n",
       " 'woutersen': {'Woutersen'},\n",
       " 'andrew': {'Andrew'},\n",
       " 'chesson': {'Chesson'},\n",
       " 'pier': {'Pier'},\n",
       " 'sandro': {'Sandro'},\n",
       " 'cocconcelli': {'Cocconcelli'},\n",
       " 'robert': {'Robert'},\n",
       " 'john': {'John'},\n",
       " 'wallac': {'Wallace'},\n",
       " 'guido': {'Guido'},\n",
       " 'rychen': {'Rychen'},\n",
       " 'rosella': {'Rosella'},\n",
       " 'brozzi': {'Brozzi'},\n",
       " 'saarela': {'Saarela'},\n",
       " 'abstract': {'Abstract'},\n",
       " 'follow': {'Following', 'followed', 'following'},\n",
       " 'request': {'Request', 'request'},\n",
       " 'from': {'from'},\n",
       " 'the': {'The', 'the'},\n",
       " 'european': {'European'},\n",
       " 'commiss': {'Commission'},\n",
       " 'was': {'was'},\n",
       " 'ask': {'asked'},\n",
       " 'to': {'To', 'to'},\n",
       " 'deliv': {'deliver'},\n",
       " 'when': {'when'},\n",
       " 'at': {'at'},\n",
       " 'minimum': {'minimum'},\n",
       " 'dose': {'dose'},\n",
       " 'coloni': {'Colony', 'colony'},\n",
       " 'form': {'Forming', 'form', 'forming'},\n",
       " 'unit': {'Unit', 'unit', 'units'},\n",
       " 'cfu': {'CFU'},\n",
       " 'per': {'per'},\n",
       " 'day': {'day', 'days'},\n",
       " 'is': {'is'},\n",
       " 'prepar': {'preparation'},\n",
       " 'viabl': {'viable'},\n",
       " 'cell': {'cell', 'cells'},\n",
       " 'this': {'This', 'this'},\n",
       " 'speci': {'species'},\n",
       " 'consid': {'considered', 'considers'},\n",
       " 'by': {'by'},\n",
       " 'food': {'Food', 'food', 'foods'},\n",
       " 'author': {'Authority'},\n",
       " 'be': {'be'},\n",
       " 'suitabl': {'suitable'},\n",
       " 'qualifi': {'Qualified', 'qualified'},\n",
       " 'presumpt': {'Presumption', 'presumption'},\n",
       " 'qps': {'QPS'},\n",
       " 'approach': {'approach'},\n",
       " 'establish': {'established', 'establishes', 'establishing'},\n",
       " 'target': {'target'},\n",
       " 'environ': {'environment'},\n",
       " 'activ': {'active'},\n",
       " 'agent': {'agent', 'agents'},\n",
       " 'fulfil': {'fulfils'},\n",
       " 'requir': {'requirements', 'requires'},\n",
       " 'assess': {'Assessment', 'assess', 'assessment'},\n",
       " 'consequ': {'Consequently'},\n",
       " 'absenc': {'absence'},\n",
       " 'concern': {'concerning', 'concerns'},\n",
       " 'other': {'other'},\n",
       " 'compon': {'components'},\n",
       " 'presum': {'presumed'},\n",
       " 'safe': {'safe'},\n",
       " 'should': {'should'},\n",
       " 'potenti': {'potential', 'potentially'},\n",
       " 'respiratori': {'respiratory'},\n",
       " 'sensitis': {'sensitisation', 'sensitiser'},\n",
       " 'data': {'Data', 'data'},\n",
       " 'can': {'can'},\n",
       " 'not': {'not'},\n",
       " 'conclud': {'conclude'},\n",
       " 'irrit': {'irritancy', 'irritation'},\n",
       " 'skin': {'skin'},\n",
       " 'eye': {'eyes'},\n",
       " 'it': {'It', 'it', 'its'},\n",
       " 'dermal': {'dermal'},\n",
       " 'posit': {'position'},\n",
       " 'publish': {'published'},\n",
       " 'wiley': {'Wiley'},\n",
       " 'son': {'Sons'},\n",
       " 'ltd': {'Ltd'},\n",
       " 'behalf': {'behalf'},\n",
       " 'keyword': {'Keywords'},\n",
       " 'zootechn': {'zootechnical'},\n",
       " 'requestor': {'Requestor'},\n",
       " 'question': {'Question'},\n",
       " 'number': {'number', 'numbers'},\n",
       " 'correspond': {'Correspondence', 'corresponding'},\n",
       " 'ej': {'ej'},\n",
       " 'member': {'Member', 'members'},\n",
       " 'acknowledg': {'Acknowledgements'},\n",
       " 'wish': {'wishes'},\n",
       " 'thank': {'thank'},\n",
       " 'support': {'support'},\n",
       " 'provid': {'provided'},\n",
       " 'output': {'output'},\n",
       " 'montserrat': {'Montserrat'},\n",
       " 'anguita': {'Anguita'},\n",
       " 'jaum': {'Jaume'},\n",
       " 'galobart': {'Galobart'},\n",
       " 'lucilla': {'Lucilla'},\n",
       " 'gregoretti': {'Gregoretti'},\n",
       " 'matteo': {'Matteo'},\n",
       " 'lorenzo': {'Lorenzo'},\n",
       " 'innocenti': {'Innocenti'},\n",
       " 'jordi': {'Jordi'},\n",
       " 'suggest': {'Suggested'},\n",
       " 'citat': {'citation'},\n",
       " 'v': {'V'},\n",
       " 'g': {'G', 'g'},\n",
       " 'ml': {'ML'},\n",
       " 'h': {'H'},\n",
       " 'b': {'B', 'b'},\n",
       " 'm': {'M'},\n",
       " 'l': {'L'},\n",
       " 'opez': {'opez'},\n",
       " 's': {'S', 's'},\n",
       " 'f': {'F'},\n",
       " 'y': {'Y'},\n",
       " 're': {'RE'},\n",
       " 'r': {'R'},\n",
       " 'ps': {'PS'},\n",
       " 'rj': {'RJ'},\n",
       " 'pp': {'pp'},\n",
       " 'https': {'https'},\n",
       " 'issn': {'ISSN'},\n",
       " 'an': {'an'},\n",
       " 'open': {'open'},\n",
       " 'access': {'access', 'accession'},\n",
       " 'articl': {'Article', 'article'},\n",
       " 'under': {'under'},\n",
       " 'term': {'Terms', 'terms'},\n",
       " 'creativ': {'Creative'},\n",
       " 'common': {'Commons'},\n",
       " 'licens': {'License'},\n",
       " 'which': {'which'},\n",
       " 'permit': {'permits'},\n",
       " 'distribut': {'distribution'},\n",
       " 'ani': {'any'},\n",
       " 'medium': {'medium'},\n",
       " 'origin': {'original'},\n",
       " 'work': {'work'},\n",
       " 'proper': {'properly'},\n",
       " 'cite': {'cited'},\n",
       " 'no': {'No', 'no'},\n",
       " 'modif': {'modifications'},\n",
       " 'adapt': {'adaptations'},\n",
       " 'are': {'are'},\n",
       " 'made': {'made'},\n",
       " 'public': {'publication'},\n",
       " 'agenc': {'agency'},\n",
       " 'union': {'Union'},\n",
       " 'tabl': {'Table'},\n",
       " 'content': {'content', 'contents'},\n",
       " 'introduct': {'Introduction', 'introduction'},\n",
       " 'background': {'Background'},\n",
       " 'refer': {'Reference', 'References', 'reference', 'referred'},\n",
       " 'inform': {'information'},\n",
       " 'methodolog': {'Methodologies', 'methodologies', 'methodology'},\n",
       " 'characteris': {'Characterisation', 'characterisation'},\n",
       " 'stabil': {'Stability', 'stability'},\n",
       " 'homogen': {'homogeneity', 'homogeneously'},\n",
       " 'condit': {'Conditions', 'conditions'},\n",
       " 'user': {'user', 'users'},\n",
       " 'conclus': {'Conclusions', 'conclusions', 'conclusively'},\n",
       " 'monitor': {'monitored', 'monitoring'},\n",
       " 'document': {'Documentation', 'documents'},\n",
       " 'chronolog': {'Chronology'},\n",
       " 'abbrevi': {'Abbreviations'},\n",
       " 'annex': {'Annex', 'Annexes'},\n",
       " 'execut': {'Executive'},\n",
       " 'summari': {'Summary'},\n",
       " 'evalu': {'Evaluation', 'evaluating'},\n",
       " 'report': {'Report', 'report', 'reporting', 'reports'},\n",
       " 'laboratori': {'Laboratories', 'Laboratory'},\n",
       " 'method': {'Method', 'Methods', 'method', 'methods'},\n",
       " 'analysi': {'Analysis', 'analysis'},\n",
       " 'regul': {'Regulation', 'regulated'},\n",
       " 'ec': {'EC'},\n",
       " 'rule': {'rules'},\n",
       " 'govern': {'governing'},\n",
       " 'communiti': {'Community'},\n",
       " 'authoris': {'Authorisation', 'authorisation', 'authorised'},\n",
       " 'nutrit': {'nutrition'},\n",
       " 'particular': {'particular', 'particulars'},\n",
       " 'that': {'that'},\n",
       " 'lay': {'laying', 'lays'},\n",
       " 'down': {'down'},\n",
       " 'person': {'person'},\n",
       " 'seek': {'seeking'},\n",
       " 'new': {'new'},\n",
       " 'shall': {'shall'},\n",
       " 'submit': {'Submitted', 'submit', 'submitted'},\n",
       " 'applic': {'Applicant',\n",
       "  'Application',\n",
       "  'applicant',\n",
       "  'application',\n",
       "  'applications'},\n",
       " 'accord': {'According', 'accordance', 'according'},\n",
       " 'with': {'with'},\n",
       " 'receiv': {'received'},\n",
       " 'nbf': {'NBF'},\n",
       " 'lane': {'Lanes'},\n",
       " 'categori': {'category'},\n",
       " 'function': {'functional'},\n",
       " 'group': {'group'},\n",
       " 'gut': {'gut'},\n",
       " 'flora': {'flora'},\n",
       " 'stabilis': {'stabilisers'},\n",
       " 'forward': {'forwarded'},\n",
       " 'were': {'were'},\n",
       " 'valid': {'valid', 'validated', 'validation'},\n",
       " 'may': {'May'},\n",
       " 'after': {'after'},\n",
       " 'verifi': {'verified', 'verifying'},\n",
       " 'undertak': {'undertake'},\n",
       " 'order': {'order'},\n",
       " 'determin': {'determine', 'determined'},\n",
       " 'whether': {'whether'},\n",
       " 'compli': {'complies'},\n",
       " 'laid': {'laid'},\n",
       " 'propos': {'proposed'},\n",
       " 'see': {'see'},\n",
       " 'section': {'Section'},\n",
       " 'contain': {'containing', 'contains'},\n",
       " 'has': {'has'},\n",
       " 'been': {'been'},\n",
       " 'previous': {'previously'},\n",
       " 'present': {'present', 'presented'},\n",
       " 'base': {'Based', 'based'},\n",
       " 'technic': {'Technical', 'technical'},\n",
       " 'eurl': {'EURL'},\n",
       " 'relat': {'relates'},\n",
       " 'control': {'control'},\n",
       " 'found': {'found'},\n",
       " 'line': {'line'},\n",
       " 'principl': {'principles'},\n",
       " 'relev': {'relevance', 'relevant'},\n",
       " 'guidanc': {'Guidance', 'guidance'},\n",
       " 'toler': {'Tolerance', 'tolerance'},\n",
       " 'studi': {'studies', 'study'},\n",
       " 'bacteri': {'bacterial'},\n",
       " 'suscept': {'susceptibility'},\n",
       " 'antimicrobi': {'antimicrobial', 'antimicrobials'},\n",
       " 'human': {'human'},\n",
       " 'veterinari': {'veterinary'},\n",
       " 'import': {'importance'},\n",
       " 'microorgan': {'microorganisms'},\n",
       " 'organ': {'organisms'},\n",
       " 'intend': {'intended'},\n",
       " 'exert': {'exert'},\n",
       " 'benefici': {'beneficial'},\n",
       " 'effect': {'effects'},\n",
       " 'their': {'their'},\n",
       " 'gastrointestin': {'gastrointestinal'},\n",
       " 'tract': {'tract'},\n",
       " 'lead': {'lead', 'leading'},\n",
       " 'increas': {'increase'},\n",
       " 'faecal': {'faecal'},\n",
       " 'consist': {'consist', 'consistency', 'consisting', 'consists'},\n",
       " 'dossier': {'Dossier', 'dossier', 'dossiers'},\n",
       " 'full': {'full'},\n",
       " 'avail': {'available'},\n",
       " 'websit': {'website'},\n",
       " 'search': {'search'},\n",
       " 'isol': {'isolated'},\n",
       " 'faec': {'faeces'},\n",
       " 'deposit': {'deposited'},\n",
       " 'deutsch': {'Deutsche'},\n",
       " 'sammlung': {'Sammlung'},\n",
       " 'von': {'von'},\n",
       " 'mikroorganismen': {'Mikroorganismen'},\n",
       " 'und': {'und'},\n",
       " 'zellkulturen': {'Zellkulturen'},\n",
       " 'dsmz': {'DSMZ'},\n",
       " 'declar': {'declares', 'declaring'},\n",
       " 'genet': {'genetic', 'genetically'},\n",
       " 'modifi': {'modified'},\n",
       " 'taxonom': {'Taxonomical'},\n",
       " 'identif': {'identification'},\n",
       " 'strain': {'strain', 'strains'},\n",
       " 'morpholog': {'morphological'},\n",
       " 'biochem': {'biochemical'},\n",
       " 'properti': {'properties'},\n",
       " 'sugar': {'sugar'},\n",
       " 'ferment': {'fermentation'},\n",
       " 'pattern': {'pattern'},\n",
       " 'analys': {'analysing'},\n",
       " 'partial': {'partial'},\n",
       " 'sequenc': {'sequence'},\n",
       " 'rrna': {'rRNA'},\n",
       " 'random': {'random', 'randomly'},\n",
       " 'amplifi': {'amplified'},\n",
       " 'polymorph': {'polymorphic'},\n",
       " 'dna': {'DNA'},\n",
       " 'rapd': {'RAPD'},\n",
       " 'test': {'test', 'tested', 'testing'},\n",
       " 'antibiot': {'antibiotic', 'antibiotics'},\n",
       " 'broth': {'broth'},\n",
       " 'microdilut': {'microdilution'},\n",
       " 'batteri': {'battery'},\n",
       " 'includ': {'include', 'included'},\n",
       " 'those': {'those'},\n",
       " 'recommend': {'recommended', 'recommends'},\n",
       " 'inhibitori': {'inhibitory'},\n",
       " 'concentr': {'concentration'},\n",
       " 'mic': {'MIC'},\n",
       " 'valu': {'value', 'values'},\n",
       " 'erythromcyin': {'erythromcyin'},\n",
       " 'clindamycin': {'clindamycin'},\n",
       " 'tetracyclin': {'tetracycline'},\n",
       " 'chloramphenicol': {'chloramphenicol'},\n",
       " 'below': {'below'},\n",
       " 'equal': {'equal'},\n",
       " 'while': {'while'},\n",
       " 'remain': {'remaining'},\n",
       " 'exceed': {'Exceedance', 'exceeded'},\n",
       " 'one': {'one'},\n",
       " 'more': {'more'},\n",
       " 'dilut': {'dilution', 'dilutions'},\n",
       " 'ampicillin': {'ampicillin'},\n",
       " 'vs': {'vs'},\n",
       " 'gentamicin': {'gentamicin'},\n",
       " 'streptomycin': {'streptomycin'},\n",
       " 'kanamycin': {'kanamycin'},\n",
       " 'fall': {'fall'},\n",
       " 'within': {'within'},\n",
       " 'normal': {'normal'},\n",
       " 'variat': {'variation'},\n",
       " 'around': {'around'},\n",
       " 'mean': {'mean'},\n",
       " 'thus': {'thus'},\n",
       " 'doe': {'does'},\n",
       " 'rais': {'raise'},\n",
       " 'elucid': {'elucidate'},\n",
       " 'natur': {'nature'},\n",
       " 'resist': {'resistance', 'resistances'},\n",
       " 'perform': {'performance', 'performed'},\n",
       " 'whole': {'whole'},\n",
       " 'genom': {'genome'},\n",
       " 'calcul': {'calculated'},\n",
       " 'size': {'size'},\n",
       " 'mb': {'Mb'},\n",
       " 'interrog': {'Interrogation', 'interrogated'},\n",
       " 'presenc': {'presence'},\n",
       " 'known': {'known'},\n",
       " 'gene': {'gene', 'genes'},\n",
       " 'code': {'coding'},\n",
       " 'compar': {'comparing'},\n",
       " 'comprehens': {'comprehensive'},\n",
       " 'databas': {'database'},\n",
       " 'card': {'CARD'},\n",
       " 'annot': {'annotation'},\n",
       " 'ardb': {'ARDB'},\n",
       " 'megar': {'MEGARes'},\n",
       " 'signific': {'significant'},\n",
       " 'match': {'matches'},\n",
       " 'detect': {'detected'},\n",
       " 'aminoglycosid': {'aminoglycoside'},\n",
       " 'acquir': {'acquired'},\n",
       " 'clinic': {'clinical'},\n",
       " 'assum': {'assumed'},\n",
       " 'caus': {'caused'},\n",
       " 'mechan': {'mechanisms'},\n",
       " 'than': {'than'},\n",
       " 'therefor': {'Therefore', 'therefore'},\n",
       " 'minim': {'minimal'},\n",
       " 'horizont': {'horizontal'},\n",
       " 'spread': {'spread'},\n",
       " 'hazard': {'Hazards', 'hazard'},\n",
       " 'grown': {'grown'},\n",
       " 'sterilis': {'sterilised'},\n",
       " 'typic': {'typical'},\n",
       " 'lactic': {'lactic'},\n",
       " 'acid': {'acid'},\n",
       " 'bacteria': {'bacteria'},\n",
       " 'then': {'then'},\n",
       " 'separ': {'separated'},\n",
       " 'growth': {'growth'},\n",
       " 'centrifug': {'centrifugation'},\n",
       " 'cryoprotect': {'Cryoprotectants'},\n",
       " 'maltodextrin': {'maltodextrin'},\n",
       " 'ad': {'added'},\n",
       " 'mix': {'mix', 'mixed', 'mixing'},\n",
       " 'ground': {'ground'},\n",
       " 'corn': {'corn'},\n",
       " 'starch': {'starch'},\n",
       " 'meet': {'meet'},\n",
       " 'specifi': {'specified'},\n",
       " 'five': {'five'},\n",
       " 'batch': {'batches'},\n",
       " 'show': {'show', 'showed'},\n",
       " 'rang': {'range'},\n",
       " 'microbi': {'Microbial'},\n",
       " 'contamin': {'contamination'},\n",
       " 'routin': {'routinely'},\n",
       " 'various': {'various'},\n",
       " 'point': {'points'},\n",
       " 'manufactur': {'Manufacturing', 'manufacturing'},\n",
       " 'process': {'process'},\n",
       " 'final': {'final'},\n",
       " 'limit': {'Limits'},\n",
       " 'set': {'set'},\n",
       " 'yeast': {'yeasts'},\n",
       " 'filament': {'filamentous'},\n",
       " 'fungi': {'fungi'},\n",
       " 'escherichia': {'Escherichia'},\n",
       " 'coli': {'coli'},\n",
       " 'absent': {'absent'},\n",
       " 'enterobacteriacea': {'Enterobacteriaceae'},\n",
       " 'salmonella': {'Salmonella'},\n",
       " 'spp': {'spp'},\n",
       " 'staphylococcus': {'Staphylococcus'},\n",
       " 'aureus': {'aureus'},\n",
       " 'anaerob': {'anaerobic'},\n",
       " 'sulfit': {'sulfite'},\n",
       " 'reduc': {'reducers'},\n",
       " 'level': {'Levels', 'levels'},\n",
       " 'aflatoxin': {'aflatoxin'},\n",
       " 'ppm': {'ppm'},\n",
       " 'mercuri': {'mercury'},\n",
       " 'cadmium': {'cadmium'},\n",
       " 'measur': {'measured'},\n",
       " 'raw': {'raw'},\n",
       " 'materi': {'materials'},\n",
       " 'onc': {'once'},\n",
       " 'year': {'year'},\n",
       " 'complianc': {'Compliance'},\n",
       " 'action': {'action'},\n",
       " 'all': {'all'},\n",
       " 'mention': {'mentioned'},\n",
       " 'impur': {'impurities'},\n",
       " 'confirm': {'confirmed'},\n",
       " 'three': {'three'},\n",
       " 'powder': {'powder'},\n",
       " 'whose': {'whose'},\n",
       " 'particl': {'particle', 'particles'},\n",
       " 'laser': {'laser'},\n",
       " 'diffract': {'diffraction'},\n",
       " 'result': {'Results'},\n",
       " 'bar': {'bars'},\n",
       " 'diamet': {'diameter'},\n",
       " 'lower': {'lower'},\n",
       " 'lm': {'lm'},\n",
       " 'dust': {'dusting'},\n",
       " 'same': {'same'},\n",
       " 'supplementari': {'Supplementary', 'supplementary'},\n",
       " 'heubach': {'Heubach'},\n",
       " 'dustomet': {'dustometer'},\n",
       " 'high': {'high'},\n",
       " 'store': {'stored'},\n",
       " 'aluminium': {'aluminium'},\n",
       " 'packag': {'packaging'},\n",
       " 'period': {'period'},\n",
       " 'month': {'month', 'months'},\n",
       " 'loss': {'Losses', 'losses'},\n",
       " 'observ': {'observed'},\n",
       " 'log': {'log'},\n",
       " 'greater': {'greater'},\n",
       " 'experi': {'experiment', 'experiments'},\n",
       " 'investig': {'investigated'},\n",
       " 'complementari': {'complementary'},\n",
       " 'describ': {'described'},\n",
       " 'microencapsul': {'microencapsulated'},\n",
       " 'tributyr': {'tributyrate'},\n",
       " 'red': {'red'},\n",
       " 'orang': {'orange'},\n",
       " 'polyphenol': {'polyphenols'},\n",
       " 'sampl': {'Samples'},\n",
       " 'capac': {'capacity'},\n",
       " 'incorpor': {'incorporated'},\n",
       " 'into': {'into'},\n",
       " 'dri': {'dry'},\n",
       " 'approxim': {'approximately'},\n",
       " 'subsampl': {'subsamples'},\n",
       " 'collect': {'collected'},\n",
       " 'subject': {'subjected', 'subjective', 'subjectively'},\n",
       " 'lactobacilli': {'lactobacilli'},\n",
       " 'count': {'Counts'},\n",
       " 'coeffici': {'coefficient'},\n",
       " 'complet': {'complete'},\n",
       " 'moistur': {'moisture'},\n",
       " 'daili': {'daily'},\n",
       " 'would': {'would'},\n",
       " 'equat': {'equate'},\n",
       " 'maximum': {'maximum'},\n",
       " 'feedingstuff': {'feedingstuffs'},\n",
       " 'two': {'Two'},\n",
       " 'but': {'but'},\n",
       " 'none': {'none'},\n",
       " 'could': {'could'},\n",
       " 'further': {'Further', 'further'},\n",
       " 'due': {'due'},\n",
       " 'inadequ': {'inadequate'},\n",
       " 'design': {'design'},\n",
       " 'case': {'case'},\n",
       " 'onli': {'only'},\n",
       " 'score': {'scores'},\n",
       " 'frequenc': {'frequency'},\n",
       " 'drawn': {'drawn'},\n",
       " 'these': {'these'},\n",
       " 'biohaz': {'BIOHAZ'},\n",
       " 'ident': {'identity'},\n",
       " 'evid': {'evidence'},\n",
       " 'view': {'view'},\n",
       " 'phenotyp': {'phenotypic'},\n",
       " 'did': {'did'},\n",
       " 'qualif': {'qualification'},\n",
       " 'met': {'met'},\n",
       " 'sinc': {'Since'},\n",
       " 'expect': {'expected'},\n",
       " 'also': {'also'},\n",
       " 'inhal': {'inhalation', 'inhaled'},\n",
       " 'toxic': {'toxicity'},\n",
       " 'have': {'have'},\n",
       " 'dusti': {'dustiness'},\n",
       " 'indic': {'indicated'},\n",
       " 'expos': {'exposed'},\n",
       " 'via': {'via'},\n",
       " 'fraction': {'fraction'},\n",
       " 'fine': {'fine'},\n",
       " 'reach': {'reach'},\n",
       " 'alveoli': {'alveoli'},\n",
       " 'given': {'Given'},\n",
       " 'proteinac': {'proteinaceous'},\n",
       " 'c': {'C', 'c'},\n",
       " 'four': {'Four'},\n",
       " 'howev': {'However'},\n",
       " 'weak': {'weaknesses'},\n",
       " 'experiment': {'experimental'},\n",
       " 'first': {'first'},\n",
       " 'short': {'short'},\n",
       " 'durat': {'duration'},\n",
       " 'insuffici': {'insufficient'},\n",
       " 'involv': {'involved'},\n",
       " 'kept': {'kept'},\n",
       " 'individu': {'individual'},\n",
       " 'hous': {'houses'},\n",
       " 'who': {'who'},\n",
       " 'train': {'trained'},\n",
       " 'sourc': {'source'},\n",
       " 'nutrient': {'nutrients'},\n",
       " 'energi': {'energy'},\n",
       " 'among': {'among'},\n",
       " 'regard': {'regarding'},\n",
       " 'appli': {'applied'},\n",
       " 'randomis': {'randomisation'},\n",
       " 'treatment': {'treatments'},\n",
       " 'blind': {'blinding'},\n",
       " 'mainten': {'maintenance'},\n",
       " 'dure': {'during'},\n",
       " 'detail': {'details'},\n",
       " 'ingest': {'ingestion'},\n",
       " 'they': {'they'},\n",
       " 'might': {'might'},\n",
       " 'had': {'had'},\n",
       " 'deviat': {'deviations'},\n",
       " 'protocol': {'protocol'},\n",
       " 'there': {'there'},\n",
       " 'need': {'need'},\n",
       " 'specif': {'specific'},\n",
       " 'plan': {'plan'},\n",
       " 'hygien': {'Hygiene', 'hygiene'},\n",
       " 'good': {'Good'},\n",
       " 'practic': {'Practice'},\n",
       " 'januari': {'January'},\n",
       " 'juli': {'July'},\n",
       " 'comment': {'Comments'},\n",
       " 'state': {'States'},\n",
       " 'parliament': {'Parliament'},\n",
       " 'council': {'Council'},\n",
       " 'oj': {'OJ'},\n",
       " 'date': {'Date'},\n",
       " 'event': {'Event'},\n",
       " 'recept': {'Reception'},\n",
       " 'mandat': {'mandate'},\n",
       " 'start': {'Start'},\n",
       " 'suspend': {'suspended'},\n",
       " 'issu': {'Issues'},\n",
       " 'clarif': {'Clarification'},\n",
       " 'teleconfer': {'teleconference'},\n",
       " 'risk': {'risk'},\n",
       " 'catalogu': {'Catalogue'},\n",
       " 'initi': {'initiatives'},\n",
       " 'end': {'End'},\n",
       " 'committe': {'Committee'},\n",
       " 'select': {'selected'},\n",
       " 'biolog': {'Biological', 'biological'},\n",
       " 'ricci': {'Ricci'},\n",
       " 'allend': {'Allende'},\n",
       " 'bolton': {'Bolton'},\n",
       " 'd': {'D'},\n",
       " 'chemali': {'Chemaly'},\n",
       " 'davi': {'Davies'},\n",
       " 'giron': {'Girones'},\n",
       " 'herman': {'Herman'},\n",
       " 'koutsoumani': {'Koutsoumanis'},\n",
       " 'k': {'K'},\n",
       " 'lindqvist': {'Lindqvist'},\n",
       " 'nørrung': {'Nørrung'},\n",
       " 'robertson': {'Robertson'},\n",
       " 'ru': {'Ru'},\n",
       " 'sanaa': {'Sanaa'},\n",
       " 'simmon': {'Simmons'},\n",
       " 'skandami': {'Skandamis'},\n",
       " 'p': {'P'},\n",
       " 'snari': {'Snary'},\n",
       " 'e': {'E'},\n",
       " 'speybroeck': {'Speybroeck'},\n",
       " 'n': {'N'},\n",
       " 'ter': {'Ter'},\n",
       " 'kuil': {'Kuile'},\n",
       " 'threlfal': {'Threlfall'},\n",
       " 'j': {'J'},\n",
       " 'wahlstrom': {'Wahlstrom'},\n",
       " 'klein': {'Klein'},\n",
       " 'deceas': {'deceased'},\n",
       " 'prieto': {'Prieto'},\n",
       " 'maradona': {'Maradona'},\n",
       " 'querol': {'Querol'},\n",
       " 'peix': {'Peixe'},\n",
       " 'suarez': {'Suarez'},\n",
       " 'je': {'JE'},\n",
       " 'sundh': {'Sundh'},\n",
       " 'i': {'I'},\n",
       " 'vlak': {'Vlak'},\n",
       " 'jm': {'JM'},\n",
       " 'barizzon': {'Barizzone'},\n",
       " 'correia': {'Correia'},\n",
       " 'heng': {'Heng'},\n",
       " 'istac': {'Istace'},\n",
       " 'lythgo': {'Lythgo'},\n",
       " 'fernandez': {'Fernandez'},\n",
       " 'esc': {'Esc'},\n",
       " 'amez': {'amez'},\n",
       " 'scienti': {'Scienti'},\n",
       " 'fic': {'fic'},\n",
       " 'updat': {'update'},\n",
       " 'list': {'list'},\n",
       " 'intent': {'intentionally'},\n",
       " 'notifi': {'notified'},\n",
       " 'usedin': {'usedin'},\n",
       " 'aquilina': {'Aquilina'},\n",
       " 'bori': {'Bories'},\n",
       " 'flachowski': {'Flachowsky'},\n",
       " 'gropp': {'Gropp'},\n",
       " 'kolar': {'Kolar'},\n",
       " 'mantovani': {'Mantovani'},\n",
       " 'wester': {'Wester'},\n",
       " 'glandorf': {'Glandorf'},\n",
       " 'aguilera': {'Aguilera'},\n",
       " 'microorganismsus': {'microorganismsused'},\n",
       " 'pfge': {'PFGE'},\n",
       " 'puls': {'Pulsed', 'pulsed'},\n",
       " 'field': {'Field', 'field'},\n",
       " 'gel': {'Gel', 'gel'},\n",
       " 'electrophoresi': {'Electrophoresis', 'electrophoresis'},\n",
       " 'current': {'current'},\n",
       " 'sought': {'sought'},\n",
       " 'market': {'marketed'},\n",
       " 'lyophilis': {'lyophilised'},\n",
       " 'administ': {'administered'},\n",
       " 'usual': {'usual'},\n",
       " 'instead': {'instead'},\n",
       " 'offici': {'official'},\n",
       " 'general': {'generally'},\n",
       " 'recognis': {'recognised'},\n",
       " 'enumer': {'enumeration'},\n",
       " 'plate': {'plate'},\n",
       " 'en': {'EN'},\n",
       " 'characterist': {'characteristics'},\n",
       " 'through': {'through'},\n",
       " 'consortium': {'consortium'},\n",
       " 'nation': {'National'},\n",
       " 'last': {'last'},\n",
       " 'amend': {'amended'},\n",
       " 'eu': {'EU'},\n",
       " 'necessari': {'necessary'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c82e0a-5ac7-474e-b21f-db081518f4d0",
   "metadata": {},
   "source": [
    "## Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7658ec76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jakobranda/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3931ad-7b80-4338-9e9b-ecb786d18e90",
   "metadata": {},
   "source": [
    "For many applications, stop words should be removed, i.e., words that occur very often and that do not contribute (significantly) to the document's semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67450dec-ea44-43e8-930e-577584ed3cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(words, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    return list(filter(lambda x: x not in stopwords, words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37b7dd7-fff3-4c7b-9d3c-cf1387030eea",
   "metadata": {},
   "source": [
    "Here, we use a list of stop words provided by nltk.  It would make sense to add certain words such as, e.g., 'doi', 'efsa', 'journal', 'panel' that occur in each of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68bef0d1-0bb5-4a55-be13-57c80ca43310",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = remove_stop_words(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcb14891-d63d-461e-b51a-184f0df2bfad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2334"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725ababe-18d4-4a94-aeb1-0bd3c803e8de",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d45d8d-e139-4277-adc4-058da8af3882",
   "metadata": {},
   "source": [
    "The Term Frequency - Inverse Document Frquency is a useful baseline representation of the text since it will weigh words according to their relevance in the text corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef03cbf-7b15-4c2b-8423-52ad9a40f889",
   "metadata": {},
   "source": [
    "## Term frequency (TF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d00772-4f94-42ea-92db-20323ba6ebd3",
   "metadata": {},
   "source": [
    "The term frequency is easy to compute, one simply counts the number of times words occur in a document, normalized by the most frequent word in the document.\n",
    "$$\n",
    "    \\mathrm{tf}(t, d) = \\frac{f_{t,d}}{\\sum_{t' \\in d} f_{t', d}}\n",
    "$$\n",
    "Here, $f_{t,d}$ is the number of times the term $t$ occurs in de document $d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e87c056-9d8d-4952-b0e2-7f9e9e2e1967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(words):\n",
    "    \"\"\" \"\"\"\n",
    "    # Element\n",
    "    term_counts = collections.defaultdict(int)\n",
    "    for word in words:\n",
    "        term_counts[word] += 1\n",
    "    total_count = sum(term_counts.values())\n",
    "    tf = collections.defaultdict(int)\n",
    "    tf.update({word: count/total_count for word, count in term_counts.items()})\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3584d17b-906c-4ae3-88d8-a93280eece5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = compute_tf(words)\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b83a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#words\n",
    "\n",
    "tf.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f71301",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(tf.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3b0ffa1d-0525-4ec2-a3f9-9ca31711e286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efsa                : 0.0313\n",
      "addit               : 0.0287\n",
      "reuteri             : 0.0236\n",
      "lactobacillus       : 0.0189\n",
      "feed                : 0.0189\n",
      "panel               : 0.0154\n",
      "safeti              : 0.0141\n",
      "dog                 : 0.0141\n",
      "use                 : 0.0141\n",
      "journal             : 0.0133\n",
      "inform              : 0.0124\n",
      "feedap              : 0.0111\n"
     ]
    }
   ],
   "source": [
    "for word, term_freq in sorted(tf.items(), key=operator.itemgetter(1), reverse=True):\n",
    "    if term_freq < 0.01: break\n",
    "    print(f'{word:20s}: {term_freq:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "07fb43b3-ad7b-4ae5-97f2-a9e1f0f59f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "683"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a9c3d9b7-f20a-4234-961a-9b9af7e9be88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_term_frequencies(file_name, stemmer=None, stopwords=None):\n",
    "    \"\"\"\n",
    "    Compute term frequencies after stemming and removal of stopwords.\n",
    "    \"\"\"\n",
    "    with open(file_name) as file:\n",
    "        text = file.read()\n",
    "    words = remove_stop_words(normalize(text, stemmer=stemmer), stopwords=stopwords)\n",
    "    return compute_tf(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c0ea6b2-02bb-4dd3-b674-ab935693e1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_measure(dir_name, measure, *args, is_verbose=False, **kvargs):\n",
    "    data_dir = pathlib.Path(dir_name)\n",
    "    measures = {}\n",
    "    for data_file in data_dir.glob('*.txt'):\n",
    "        if is_verbose:\n",
    "            print(f'processing {data_file.name}', file=sys.stderr)\n",
    "        measures[data_file.stem] = measure(data_file, *args, **kvargs)\n",
    "    return measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f67fbbae-fdba-49e6-ba7c-a66cf7855f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfs_pickle_file_name = 'data/tfs.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c417cd77-4966-4afc-b67c-15ac9df08ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File existsss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing 5524 copy.txt\n",
      "processing 5524.txt\n",
      "processing 5525.txt\n",
      "processing 5527.txt\n",
      "processing 5526.txt\n",
      "processing 5522.txt\n",
      "processing 5523.txt\n",
      "processing 5521.txt\n",
      "processing 5513.txt\n",
      "processing 5501.txt\n"
     ]
    }
   ],
   "source": [
    "tfs_picke_file = pathlib.Path(tfs_pickle_file_name)\n",
    "if tfs_picke_file.exists():\n",
    "    print(\"File exists\")\n",
    "    with open(tfs_pickle_file_name, 'rb') as pickle_file:\n",
    "        tfs = pickle.load(pickle_file)\n",
    "else:\n",
    "    print(\"File existsss\")\n",
    "    tfs = compute_measure('data/2-papers-text/', compute_term_frequencies, is_verbose=True)\n",
    "    with open('tfs.pickle', 'wb') as pickle_file:\n",
    "        pickle.dump(tfs, pickle_file, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ca3b56-9234-4acb-8078-5b163f7616d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inverse document frequency (IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170a77ca-74da-4e80-b00c-756445e95a6c",
   "metadata": {},
   "source": [
    "Once we have the term frequencies, it is easy to compute the inverse document frequency for each term.\n",
    "$$\n",
    "    \\mathrm{idf}(t, D) = \\log \\frac{|D|}{|\\{d \\in D | t \\in d\\}|}\n",
    "$$\n",
    "The corpus of ducuments is denoted by $D$, and $t$ is a term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc71f4c4-f68b-4078-8a34-e25b3d1d7c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_document_count(term, term_freqs):\n",
    "    return sum(map(lambda doc_id: 1 if term in term_freqs[doc_id] else 0, term_freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a9a9afe-9b24-402c-8296-f9d4308b8fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(term, term_freqs):\n",
    "    return math.log(len(term_freqs)/compute_document_count(term, term_freqs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6988e36-407f-4c45-af61-1907a56d6ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_document_count('efsa', tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e04750a2-b8c3-4022-b2f6-c7158fe814d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7890892406721761"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log(748/125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47cc5763-ef8d-4759-914a-c2ffabe090cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10536051565782635"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_idf('efsa', tfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e01a16-60b2-408f-92de-4f512cbfa691",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231f5dc9-5bf4-41ce-9d41-ad44a6873f68",
   "metadata": {},
   "source": [
    "The TF-IDF for a term $t$ in a document $d$ that belongs to the corpus $D$ is given by:\n",
    "\n",
    "$$\n",
    "    \\mathrm{tfidf}(t, d, D) = \\mathrm{tf}(t, d) \\cdot \\mathrm{idf}(t, D)\n",
    "$$\n",
    "\n",
    "The higher the TF-IDF value of a term in a document, the more distiinctive (or descriptive) it is for that document.  Words that occur in all documents will have a TF-IDF value equal to zero, so they are not informative.  On the other hand, words that occur in only a few documents will carry information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40536c76-5c16-4358-89a3-92ea4f98a19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf_idf(term, doc_id, term_freqs):\n",
    "    return term_freqs[doc_id][term]*compute_idf(term, term_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "94e6afca-9872-4277-999a-48c86dc09ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.032555830363669026"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_tf_idf('dog', '5524', tfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b404602-d3bd-422c-85ab-de5d246f0ccf",
   "metadata": {},
   "source": [
    "# Document vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1082688-dd23-4a04-abca-2d3bb18aa8e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Corpus terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3856fa19-6c9f-4d38-a8c1-36c7708563fd",
   "metadata": {},
   "source": [
    "In order to represent documents by TF-IDF, we need to compute a vector containing all (relevant) words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5cd21117-2cfb-4791-b3c8-fe69f5cc3b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_word_vector(tfs):\n",
    "    words = functools.reduce(lambda a, b: set(a) | b, map(lambda x: x.keys(), tfs.values()), set())\n",
    "    return list(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30e3e757-eb90-4fc7-a2a2-de90fe665b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector = compute_word_vector(tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "502e622a-e106-4f02-b05b-f799576a1073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4688"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92915872-55a3-4615-9ffe-d4f7b45b1b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ultraviolet',\n",
       " 'probabl',\n",
       " 'favour',\n",
       " 'worldwid',\n",
       " 'infecti',\n",
       " 'surfac',\n",
       " 'bulgarian',\n",
       " 'israel',\n",
       " 'either',\n",
       " 'spp',\n",
       " 'avg',\n",
       " 'arachidon',\n",
       " 'skin',\n",
       " 'set',\n",
       " 'rootworm']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4871f09f-1b76-4528-a3db-9dcf466cc337",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/word_vector.txt', 'w') as file:\n",
    "    print('\\n'.join(word_vector), file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a377ed-123e-4527-af9e-23fa9b4bc323",
   "metadata": {},
   "source": [
    "## TF-IDF document representation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa899227",
   "metadata": {},
   "source": [
    "Foreach word in whole corpus, calculate a number for the given documents describing how relevant the word is to this particular document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2696b6f5-5736-4cd8-8681-073239d3e51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_document_vector(word_vector, doc_id, term_freqs):\n",
    "    return list(map(lambda term: compute_tf_idf(term, doc_id, term_freqs), word_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7db535c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5ca7fae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ultraviolet',\n",
       " 'probabl',\n",
       " 'favour',\n",
       " 'worldwid',\n",
       " 'infecti',\n",
       " 'surfac',\n",
       " 'bulgarian',\n",
       " 'israel',\n",
       " 'either',\n",
       " 'spp',\n",
       " 'avg',\n",
       " 'arachidon',\n",
       " 'skin',\n",
       " 'set',\n",
       " 'rootworm',\n",
       " 'dusti',\n",
       " 'albedini',\n",
       " 'quito',\n",
       " 'acceler',\n",
       " 'barbottin',\n",
       " 'transfer',\n",
       " 'spmyev',\n",
       " 'vertic',\n",
       " 'year',\n",
       " 'sterilis',\n",
       " 'stipul',\n",
       " 'sensori',\n",
       " 'sympatr',\n",
       " 'glanvill',\n",
       " 'davidson',\n",
       " 'abov',\n",
       " 'istac',\n",
       " 'zeszyti',\n",
       " 'thorn',\n",
       " 'cseh',\n",
       " 'herrmann',\n",
       " 'dietel',\n",
       " 'clavibact',\n",
       " 'acetyltransferas',\n",
       " 'ortuno',\n",
       " 'triglycerid',\n",
       " 'subject',\n",
       " 'alvarez',\n",
       " 'dacus',\n",
       " 'bel',\n",
       " 'lemair',\n",
       " 'sweden',\n",
       " 'pig',\n",
       " 'vitamin',\n",
       " 'ingest',\n",
       " 'vat',\n",
       " 'tigr',\n",
       " 'yoshikawa',\n",
       " 'mcrobert',\n",
       " 'sapotaefolia',\n",
       " 'marn',\n",
       " 'cao',\n",
       " 'anov',\n",
       " 'fungi',\n",
       " 'invit',\n",
       " 'dna',\n",
       " 'entomologist',\n",
       " 'histidin',\n",
       " 'overwint',\n",
       " 'missouri',\n",
       " 'programm',\n",
       " 'tomv',\n",
       " 'hassan',\n",
       " 'nine',\n",
       " 'strength',\n",
       " 'leba',\n",
       " 'multipli',\n",
       " 'alv',\n",
       " 'ad',\n",
       " 'expect',\n",
       " 'sinc',\n",
       " 'confin',\n",
       " 'corn',\n",
       " 'gestat',\n",
       " 'assembl',\n",
       " 'establish',\n",
       " 'asynchron',\n",
       " 'onli',\n",
       " 'florida',\n",
       " 'bryxiova',\n",
       " 'step',\n",
       " 'requestor',\n",
       " 'parliament',\n",
       " 'gremmeniella',\n",
       " 'fell',\n",
       " 'project',\n",
       " 'confus',\n",
       " 'seedl',\n",
       " 'kirbi',\n",
       " 'behalf',\n",
       " 'entranc',\n",
       " 'right',\n",
       " 'autoclav',\n",
       " 'choi',\n",
       " 'formula',\n",
       " 'minerva',\n",
       " 'undergo',\n",
       " 'loss',\n",
       " 'stunt',\n",
       " 'alt',\n",
       " 'cabi',\n",
       " 'express',\n",
       " 'manganes',\n",
       " 'task',\n",
       " 'full',\n",
       " 'ongo',\n",
       " 'complementari',\n",
       " 'low',\n",
       " 'human',\n",
       " 'dev',\n",
       " 'fagund',\n",
       " 'ardb',\n",
       " 'apricot',\n",
       " 'vine',\n",
       " 'jl',\n",
       " 'hamam',\n",
       " 'wide',\n",
       " 'wash',\n",
       " 'xd',\n",
       " 'mg',\n",
       " 'noda',\n",
       " 'alveoli',\n",
       " 'eggplant',\n",
       " 'sihelsk',\n",
       " 'result',\n",
       " 'metabolit',\n",
       " 'particl',\n",
       " 'hanspet',\n",
       " 'western',\n",
       " 'alimentaria',\n",
       " 'curculionida',\n",
       " 'phytopath',\n",
       " 'zootechn',\n",
       " 'southern',\n",
       " 'subvir',\n",
       " 'summaris',\n",
       " 'allergen',\n",
       " 'mammalian',\n",
       " 'woutersen',\n",
       " 'function',\n",
       " 'fungal',\n",
       " 'chiumenti',\n",
       " 'rp',\n",
       " 'instar',\n",
       " 'insecta',\n",
       " 'buprestida',\n",
       " 'tr',\n",
       " 'inactiv',\n",
       " 'pathway',\n",
       " 'boar',\n",
       " 'cadmium',\n",
       " 'contact',\n",
       " 'spakov',\n",
       " 'chronolog',\n",
       " 'varieti',\n",
       " 'distinct',\n",
       " 'evolutionari',\n",
       " 'rodoni',\n",
       " 'promot',\n",
       " 'legisl',\n",
       " 'tissu',\n",
       " 'gfkv',\n",
       " 'factor',\n",
       " 'receipt',\n",
       " 'triticum',\n",
       " 'middl',\n",
       " 'sobemovirus',\n",
       " 'insv',\n",
       " 'grapevin',\n",
       " 'comparison',\n",
       " 'lira',\n",
       " 'colder',\n",
       " 'monogastr',\n",
       " 'parasit',\n",
       " 'polici',\n",
       " 'der',\n",
       " 'america',\n",
       " 'europ',\n",
       " 'occasion',\n",
       " 'aggreg',\n",
       " 'prieskaensi',\n",
       " 'suitabl',\n",
       " 'bruggen',\n",
       " 'strain',\n",
       " 'structur',\n",
       " 'borner',\n",
       " 'potenti',\n",
       " 'feedback',\n",
       " 'lycopersici',\n",
       " 'ellipticus',\n",
       " 'bcarv',\n",
       " 'gamav',\n",
       " 'descript',\n",
       " 'internet',\n",
       " 'master',\n",
       " 'maradona',\n",
       " 'hrmo',\n",
       " 'phytopathologia',\n",
       " 'fed',\n",
       " 'vv',\n",
       " 'nucleotid',\n",
       " 'tama',\n",
       " 'francisco',\n",
       " 'grey',\n",
       " 'piec',\n",
       " 'permit',\n",
       " 'jurgen',\n",
       " 'centralis',\n",
       " 'temperatur',\n",
       " 'fumig',\n",
       " 'clindamycin',\n",
       " 'listronotus',\n",
       " 'hsvd',\n",
       " 'prvt',\n",
       " 'hita',\n",
       " 'solitaria',\n",
       " 'pulmonari',\n",
       " 'phenotyp',\n",
       " 'variabl',\n",
       " 'till',\n",
       " 'iso',\n",
       " 'ix',\n",
       " 'olmo',\n",
       " 'gonipterus',\n",
       " 'dustomet',\n",
       " 'coat',\n",
       " 'symposium',\n",
       " 'nr',\n",
       " 'latent',\n",
       " 'zinc',\n",
       " 'fragaria',\n",
       " 'gilioli',\n",
       " 'avail',\n",
       " 'durat',\n",
       " 'ewen',\n",
       " 'march',\n",
       " 'industri',\n",
       " 'dormant',\n",
       " 'fungusfit',\n",
       " 'preliminari',\n",
       " 'made',\n",
       " 'ampelovirus',\n",
       " 'refer',\n",
       " 'scotia',\n",
       " 'core',\n",
       " 'healthprotect',\n",
       " 'resampl',\n",
       " 'gtrv',\n",
       " 'impatien',\n",
       " 'conc',\n",
       " 'fao',\n",
       " 'idaeovirus',\n",
       " 'determin',\n",
       " 'screenshot',\n",
       " 'undertaken',\n",
       " 'median',\n",
       " 'across',\n",
       " 'federici',\n",
       " 'mention',\n",
       " 'rout',\n",
       " 'precis',\n",
       " 'lod',\n",
       " 'fulfil',\n",
       " 'introduc',\n",
       " 'blvs',\n",
       " 'chiloensi',\n",
       " 'better',\n",
       " 'packag',\n",
       " 'faec',\n",
       " 'agrilus',\n",
       " 'lambert',\n",
       " 'phloem',\n",
       " 'daili',\n",
       " 'moravia',\n",
       " 'max',\n",
       " 'liu',\n",
       " 'zeller',\n",
       " 'arginin',\n",
       " 'delta',\n",
       " 'schrijvera',\n",
       " 'maguet',\n",
       " 'contamin',\n",
       " 'assvd',\n",
       " 'bulletin',\n",
       " 'option',\n",
       " 'vehicl',\n",
       " 'skidmor',\n",
       " 'canker',\n",
       " 'laghezza',\n",
       " 'context',\n",
       " 'iysp',\n",
       " 'truncat',\n",
       " 'ne',\n",
       " 'riva',\n",
       " 'preparatori',\n",
       " 'donn',\n",
       " 'crsv',\n",
       " 'usedin',\n",
       " 'yo',\n",
       " 'fifth',\n",
       " 'free',\n",
       " 'praper',\n",
       " 'juan',\n",
       " 'bud',\n",
       " 'vijtewa',\n",
       " 'detect',\n",
       " 'bvz',\n",
       " 'mousavi',\n",
       " 'less',\n",
       " 'tospovirus',\n",
       " 'actin',\n",
       " 'move',\n",
       " 'kit',\n",
       " 'submiss',\n",
       " 'kaponi',\n",
       " 'alnifolia',\n",
       " 'cj',\n",
       " 'uluba',\n",
       " 'partnership',\n",
       " 'gm',\n",
       " 'fructos',\n",
       " 'sa',\n",
       " 'undertak',\n",
       " 'label',\n",
       " 'itali',\n",
       " 'anticip',\n",
       " 'angel',\n",
       " 'rootstock',\n",
       " 'kumamotoensi',\n",
       " 'reus',\n",
       " 'mykiss',\n",
       " 'impli',\n",
       " 'karssen',\n",
       " 'westrick',\n",
       " 'winter',\n",
       " 'klein',\n",
       " 'k',\n",
       " 'rplv',\n",
       " 'eve',\n",
       " 'da',\n",
       " 'differ',\n",
       " 'necessari',\n",
       " 'holder',\n",
       " 'metabol',\n",
       " 'identi',\n",
       " 'bernadri',\n",
       " 'came',\n",
       " 'aleurocantus',\n",
       " 'kosztra',\n",
       " 'berniak',\n",
       " 'barrus',\n",
       " 'xxxxx',\n",
       " 'evid',\n",
       " 'avocado',\n",
       " 'fm',\n",
       " 'bresad',\n",
       " 'rf',\n",
       " 'particular',\n",
       " 'distribut',\n",
       " 'infant',\n",
       " 'bello',\n",
       " 'japan',\n",
       " 'hm',\n",
       " 'stephan',\n",
       " 'bitanc',\n",
       " 'per',\n",
       " 'urinari',\n",
       " 'ak',\n",
       " 'mj',\n",
       " 'tmv',\n",
       " 'faggioli',\n",
       " 'rapid',\n",
       " 'inspir',\n",
       " 'dioxin',\n",
       " 'tfdav',\n",
       " 'west',\n",
       " 'laser',\n",
       " 'note',\n",
       " 'weight',\n",
       " 'prebiot',\n",
       " 'nation',\n",
       " 'salama',\n",
       " 'brambl',\n",
       " 'nagata',\n",
       " 'phoenicolasius',\n",
       " 'scientif',\n",
       " 'abstract',\n",
       " 'effici',\n",
       " 'mojca',\n",
       " 'thus',\n",
       " 'rasp',\n",
       " 'plummer',\n",
       " 'vitisreason',\n",
       " 'petkova',\n",
       " 'ear',\n",
       " 'trr',\n",
       " 'lanzoni',\n",
       " 'gropp',\n",
       " 'toward',\n",
       " 'ec',\n",
       " 'offici',\n",
       " 'neri',\n",
       " 'enamovirus',\n",
       " 'manur',\n",
       " 'pear',\n",
       " 'dye',\n",
       " 'wisconsin',\n",
       " 'anthonomus',\n",
       " 'immunotoxicolog',\n",
       " 'fg',\n",
       " 'springer',\n",
       " 'lavina',\n",
       " 'stabilis',\n",
       " 'aureus',\n",
       " 'call',\n",
       " 'spark',\n",
       " 'eight',\n",
       " 'grablovirus',\n",
       " 'scan',\n",
       " 'sheet',\n",
       " 'pdv',\n",
       " 'great',\n",
       " 'agrosci',\n",
       " 'rc',\n",
       " 'erythromcyin',\n",
       " 'brozzi',\n",
       " 'anastassiadou',\n",
       " 'fi',\n",
       " 'consign',\n",
       " 'lebanon',\n",
       " 'ctp',\n",
       " 'squash',\n",
       " 'entomologici',\n",
       " 'lp',\n",
       " 'scotland',\n",
       " 'miguel',\n",
       " 'llc',\n",
       " 'tuberosum',\n",
       " 'sydow',\n",
       " 'ev',\n",
       " 'dimitra',\n",
       " 'anton',\n",
       " 'buckleyi',\n",
       " 'et',\n",
       " 'health',\n",
       " 'larger',\n",
       " 'doi',\n",
       " 'forag',\n",
       " 'pospiviroid',\n",
       " 'administ',\n",
       " 'prvf',\n",
       " 'rapese',\n",
       " 'sacchi',\n",
       " 'naupactus',\n",
       " 'texana',\n",
       " 'nj',\n",
       " 'north',\n",
       " 'kati',\n",
       " 'answer',\n",
       " 'fri',\n",
       " 'creativ',\n",
       " 'lucien',\n",
       " 'record',\n",
       " 'mammatum',\n",
       " 'phoresi',\n",
       " 'boubouraka',\n",
       " 'lake',\n",
       " 'putat',\n",
       " 'liang',\n",
       " 'totivirida',\n",
       " 'paprstein',\n",
       " 'taxonom',\n",
       " 'remark',\n",
       " 'filament',\n",
       " 'copper',\n",
       " 'cvs',\n",
       " 'normal',\n",
       " 'ficat',\n",
       " 'mekuria',\n",
       " 'uncertainti',\n",
       " 'adf',\n",
       " 'renew',\n",
       " 'come',\n",
       " 'densiti',\n",
       " 'speckl',\n",
       " 'yaegashi',\n",
       " 'manag',\n",
       " 'spleen',\n",
       " 'lecont',\n",
       " 'famili',\n",
       " 'busetto',\n",
       " 'broadleav',\n",
       " 'idaeus',\n",
       " 'accompani',\n",
       " 'shen',\n",
       " 'sp',\n",
       " 'author',\n",
       " 'mercuri',\n",
       " 'simmon',\n",
       " 'lay',\n",
       " 'chromium',\n",
       " 'lecoq',\n",
       " 'approxim',\n",
       " 'life',\n",
       " 'canadensi',\n",
       " 'wallingford',\n",
       " 'absolut',\n",
       " 'communi',\n",
       " 'percentag',\n",
       " 'miozzi',\n",
       " 'zj',\n",
       " 'nil',\n",
       " 'main',\n",
       " 'leptinotarsa',\n",
       " 'curran',\n",
       " 'miner',\n",
       " 'tymovirus',\n",
       " 'quercus',\n",
       " 'japanes',\n",
       " 'ferul',\n",
       " 'tizard',\n",
       " 'schrader',\n",
       " 'five',\n",
       " 'light',\n",
       " 'bake',\n",
       " 'expos',\n",
       " 'ten',\n",
       " 'offic',\n",
       " 'dsmz',\n",
       " 'reduct',\n",
       " 'boil',\n",
       " 'deem',\n",
       " 'globe',\n",
       " 'edoardo',\n",
       " 'extent',\n",
       " 'aguilera',\n",
       " 'dietari',\n",
       " 'evan',\n",
       " 'recalcul',\n",
       " 'catalogu',\n",
       " 'uptak',\n",
       " 'dell',\n",
       " 'thymus',\n",
       " 'proceed',\n",
       " 'tv',\n",
       " 'lim',\n",
       " 'fleck',\n",
       " 'radioact',\n",
       " 'phylogenet',\n",
       " 'amelanchi',\n",
       " 'epb',\n",
       " 'pyrifolia',\n",
       " 'plus',\n",
       " 'dumont',\n",
       " 'js',\n",
       " 'atropunctata',\n",
       " 'filter',\n",
       " 'lafortezza',\n",
       " 'ho',\n",
       " 'gossypii',\n",
       " 'deliv',\n",
       " 'effort',\n",
       " 'transit',\n",
       " 'gmos',\n",
       " 'krizbai',\n",
       " 'methionin',\n",
       " 'xiao',\n",
       " 'tanumihardjo',\n",
       " 'east',\n",
       " 'amez',\n",
       " 'throughout',\n",
       " 'congesta',\n",
       " 'modif',\n",
       " 'querol',\n",
       " 'specialist',\n",
       " 'satisfi',\n",
       " 'algerian',\n",
       " 'recombin',\n",
       " 'abnorm',\n",
       " 'europhyt',\n",
       " 'pyrenaica',\n",
       " 'afcvd',\n",
       " 'platypodida',\n",
       " 'wandelt',\n",
       " 'ee',\n",
       " 'priori',\n",
       " 'never',\n",
       " 'tular',\n",
       " 'matter',\n",
       " 'cold',\n",
       " 'castanea',\n",
       " 'count',\n",
       " 'contributor',\n",
       " 'aec',\n",
       " 'pollut',\n",
       " 'uniqu',\n",
       " 'vy',\n",
       " 'chanc',\n",
       " 'finetti',\n",
       " 'josep',\n",
       " 'geoloc',\n",
       " 'herman',\n",
       " 'scienti',\n",
       " 'mare',\n",
       " 'gvcv',\n",
       " 'oversea',\n",
       " 'progeni',\n",
       " 'synchron',\n",
       " 'porcin',\n",
       " 'suffert',\n",
       " 'chloe',\n",
       " 'alphapartitivirus',\n",
       " 'section',\n",
       " 'bacteri',\n",
       " 'lactobacillus',\n",
       " 'sahlberg',\n",
       " 'distinguish',\n",
       " 'toddler',\n",
       " 'crickmor',\n",
       " 'growth',\n",
       " 'conifer',\n",
       " 'spectrophotometr',\n",
       " 'seen',\n",
       " 'quizalofop',\n",
       " 'undergon',\n",
       " 'szabo',\n",
       " 'extrapol',\n",
       " 'em',\n",
       " 'heubach',\n",
       " 'higher',\n",
       " 'groov',\n",
       " 'graphocephala',\n",
       " 'autonel',\n",
       " 'rumex',\n",
       " 'mucos',\n",
       " 'barrel',\n",
       " 'consensus',\n",
       " 'marianna',\n",
       " 'insert',\n",
       " 'wright',\n",
       " 'babini',\n",
       " 'perenni',\n",
       " 'nave',\n",
       " 'alpha',\n",
       " 'ithaca',\n",
       " 'lesemann',\n",
       " 'facilit',\n",
       " 'ab',\n",
       " 'portugues',\n",
       " 'lepidopteran',\n",
       " 'arx',\n",
       " 'appli',\n",
       " 'length',\n",
       " 'preform',\n",
       " 'wineberri',\n",
       " 'sediment',\n",
       " 'elimin',\n",
       " 'fagus',\n",
       " 'il',\n",
       " 'ew',\n",
       " 'alessia',\n",
       " 'oxysporum',\n",
       " 'hr',\n",
       " 'cheravirus',\n",
       " 'elli',\n",
       " 'pityogen',\n",
       " 'deposit',\n",
       " 'gennaro',\n",
       " 'elisa',\n",
       " 'handl',\n",
       " 'ac',\n",
       " 'furthermor',\n",
       " 'lane',\n",
       " 'foliag',\n",
       " 'pepper',\n",
       " 'feranec',\n",
       " 'twist',\n",
       " 'appropri',\n",
       " 'whey',\n",
       " 'koenig',\n",
       " 'microorganismsus',\n",
       " 'microbi',\n",
       " 'sugar',\n",
       " 'echa',\n",
       " 'eosinophil',\n",
       " 'spectrum',\n",
       " 'annex',\n",
       " 'subash',\n",
       " 'window',\n",
       " 'poorer',\n",
       " 'blackwel',\n",
       " 'cn',\n",
       " 'inventori',\n",
       " 'pritchard',\n",
       " 'yang',\n",
       " 'alrsv',\n",
       " 'pierc',\n",
       " 'cercoseptoria',\n",
       " 'monochamus',\n",
       " 'sun',\n",
       " 'ige',\n",
       " 'process',\n",
       " 'sammlung',\n",
       " 'pvx',\n",
       " 'trace',\n",
       " 'deviat',\n",
       " 'product',\n",
       " 'isbn',\n",
       " 'progress',\n",
       " 'acid',\n",
       " 'brewer',\n",
       " 'season',\n",
       " 'serotina',\n",
       " 'interrupt',\n",
       " 'verma',\n",
       " 'abbrevi',\n",
       " 'territori',\n",
       " 'gentamicin',\n",
       " 'kingdom',\n",
       " 'neu',\n",
       " 'allen',\n",
       " 'ren',\n",
       " 'esc',\n",
       " 'flavour',\n",
       " 'blanchard',\n",
       " 'incid',\n",
       " 'andean',\n",
       " 'vicent',\n",
       " 'nova',\n",
       " 'condit',\n",
       " 'valin',\n",
       " 'morbosa',\n",
       " 'dormanc',\n",
       " 'perditus',\n",
       " 'hedg',\n",
       " 'signoret',\n",
       " 'nectarin',\n",
       " 'walsh',\n",
       " 'hs',\n",
       " 'edibl',\n",
       " 'servic',\n",
       " 'equal',\n",
       " 'cprv',\n",
       " 'primo',\n",
       " 'mountain',\n",
       " 'diverg',\n",
       " 'kidney',\n",
       " 'tannat',\n",
       " 'dr',\n",
       " 'ferment',\n",
       " 'therefor',\n",
       " 'escap',\n",
       " 'sensu',\n",
       " 'moon',\n",
       " 'crv',\n",
       " 'ramo',\n",
       " 'mair',\n",
       " 'subfamili',\n",
       " 'dhr',\n",
       " 'non',\n",
       " 'madeira',\n",
       " 'blmov',\n",
       " 'month',\n",
       " 'schan',\n",
       " 'blouin',\n",
       " 'cxl',\n",
       " 'technolog',\n",
       " 'man',\n",
       " 'predajna',\n",
       " 'immatur',\n",
       " 'dermal',\n",
       " 'eq',\n",
       " 'bird',\n",
       " 'vineyard',\n",
       " 'signatus',\n",
       " 'giron',\n",
       " 'cho',\n",
       " 'align',\n",
       " 'stepwis',\n",
       " 'km',\n",
       " 'azor',\n",
       " 'acleri',\n",
       " 'biosynthesi',\n",
       " 'concept',\n",
       " 'labil',\n",
       " 'cytorhabdovirus',\n",
       " 'minut',\n",
       " 'birgit',\n",
       " 'bcmv',\n",
       " 'formul',\n",
       " 'pat',\n",
       " 'croft',\n",
       " 'phytopatholog',\n",
       " 'deionis',\n",
       " 'broer',\n",
       " 'urticaria',\n",
       " 'nl',\n",
       " 'firbank',\n",
       " 'louisiana',\n",
       " 'kanamycin',\n",
       " 'consecut',\n",
       " 'discov',\n",
       " 'larg',\n",
       " 'cvb',\n",
       " 'lamb',\n",
       " 'recent',\n",
       " 'gvt',\n",
       " 'padilla',\n",
       " 'laurifolia',\n",
       " 'situat',\n",
       " 'bezzi',\n",
       " 'pattern',\n",
       " 'mesenterium',\n",
       " 'starter',\n",
       " 'multitud',\n",
       " 'mazzei',\n",
       " 'us',\n",
       " 'releas',\n",
       " 'cultiv',\n",
       " 'chiavetta',\n",
       " 'expert',\n",
       " 'approv',\n",
       " 'molecul',\n",
       " 'inconclus',\n",
       " 'substanti',\n",
       " 'raw',\n",
       " 'announc',\n",
       " 'burgyan',\n",
       " 'provision',\n",
       " 'ai',\n",
       " 'dn',\n",
       " 'southeastern',\n",
       " 'error',\n",
       " 'chooi',\n",
       " 'fisher',\n",
       " 'stockhoff',\n",
       " 'tamv',\n",
       " 'pamv',\n",
       " 'randl',\n",
       " 'domestica',\n",
       " 'gt',\n",
       " 'virginiana',\n",
       " 'epsg',\n",
       " 'kawai',\n",
       " 'word',\n",
       " 'time',\n",
       " 'aab',\n",
       " 'proper',\n",
       " 'allow',\n",
       " 'macdonald',\n",
       " 'pascher',\n",
       " 'proxim',\n",
       " 'young',\n",
       " 'rug',\n",
       " 'nuclear',\n",
       " 'muscl',\n",
       " 'saf',\n",
       " 'threlfal',\n",
       " 'rg',\n",
       " 'locus',\n",
       " 'cover',\n",
       " 'tandem',\n",
       " 'inappropri',\n",
       " 'fccv',\n",
       " 'gavag',\n",
       " 'ruiz',\n",
       " 'phytoparasitica',\n",
       " 'fejer',\n",
       " 'germansvill',\n",
       " 'write',\n",
       " 'aschistonyx',\n",
       " 'vlak',\n",
       " 'figur',\n",
       " 'nazional',\n",
       " 'probe',\n",
       " 'flake',\n",
       " 'experi',\n",
       " 'mauri',\n",
       " 'nd',\n",
       " 'arab',\n",
       " 'park',\n",
       " 'goat',\n",
       " 'joint',\n",
       " 'law',\n",
       " 'frontier',\n",
       " 'ginv',\n",
       " 'pribylova',\n",
       " 'widest',\n",
       " 'amount',\n",
       " 'nomin',\n",
       " 'segreg',\n",
       " 'caffier',\n",
       " 'netherland',\n",
       " 'eastern',\n",
       " 'plos',\n",
       " 'austin',\n",
       " 'smaller',\n",
       " 'jfd',\n",
       " 'catalog',\n",
       " 'z',\n",
       " 'ilsi',\n",
       " 'lentdeck',\n",
       " 'enter',\n",
       " 'verifi',\n",
       " 'confer',\n",
       " 'hercynia',\n",
       " 'mariana',\n",
       " 'harvest',\n",
       " 'phi',\n",
       " 'pen',\n",
       " 'london',\n",
       " 'gcmv',\n",
       " 'redund',\n",
       " 'assum',\n",
       " 'recognis',\n",
       " 'masoumi',\n",
       " 'pearlsman',\n",
       " 'glvd',\n",
       " 'unidentifi',\n",
       " 'orsolya',\n",
       " 'bilineatus',\n",
       " 'dow',\n",
       " 'microorgan',\n",
       " 'flowabl',\n",
       " 'clrv',\n",
       " 'antimicrobi',\n",
       " 'r',\n",
       " 'greyscal',\n",
       " 'unrel',\n",
       " 'endpoint',\n",
       " 'compos',\n",
       " 'stamp',\n",
       " 'digiaro',\n",
       " 'mf',\n",
       " 'minutus',\n",
       " 'increas',\n",
       " 'host',\n",
       " 'lindqvist',\n",
       " 'fourteen',\n",
       " 'agranowski',\n",
       " 'visual',\n",
       " 'voragen',\n",
       " 'elbeaino',\n",
       " 'gergerich',\n",
       " 'pecmv',\n",
       " 'way',\n",
       " 'botterman',\n",
       " 'tepovirus',\n",
       " 'marcon',\n",
       " 'bg',\n",
       " 'memoir',\n",
       " 'individu',\n",
       " 'untransform',\n",
       " 'veerakon',\n",
       " 'macquair',\n",
       " 'estev',\n",
       " 'anyi',\n",
       " 'redleaf',\n",
       " 'serolog',\n",
       " 'quadrigibbus',\n",
       " 'agronom',\n",
       " 'supervis',\n",
       " 'lang',\n",
       " 'tbrv',\n",
       " 'sown',\n",
       " 'bpyv',\n",
       " 'stmr',\n",
       " 'septoria',\n",
       " 'brnv',\n",
       " 'thakur',\n",
       " 'mn',\n",
       " 'approach',\n",
       " 'blend',\n",
       " 'schenkl',\n",
       " 'virescen',\n",
       " 'aflatoxin',\n",
       " 'decid',\n",
       " 'glrsv',\n",
       " 'americanum',\n",
       " 'susaimuthu',\n",
       " 'rplcv',\n",
       " 'groundwat',\n",
       " 'kot',\n",
       " 'mycogen',\n",
       " 'geograph',\n",
       " 'insect',\n",
       " 'betaflexivirus',\n",
       " 'mckenna',\n",
       " 'inhibitor',\n",
       " 'tfeu',\n",
       " 'hong',\n",
       " 'rvf',\n",
       " 'milona',\n",
       " 'especi',\n",
       " 'personnel',\n",
       " ...]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eddb7a15-5b2f-4606-9814-c0634f736a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vec = compute_document_vector(word_vector, '5524', tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f3480034-5f15-49bf-bb48-66710d3e6748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4688"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "103a6cf3-18bd-4e63-ab4d-86d989e0cad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0002969782264609877,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0015703354445144046,\n",
       " 9.560563466761344e-05,\n",
       " 0.0]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vec[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f0f897-96ea-4cf6-8af3-46b39232466e",
   "metadata": {},
   "source": [
    "This vector is of course sparse since the document will have term frequency 0 for most terms in the word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "63d7a9bf-391c-4cba-9dd4-6e81f3df594e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "koger\n",
      "5524 copy\n",
      "koger\n",
      "5524\n",
      "koger\n",
      "5525\n",
      "koger\n",
      "5527\n",
      "koger\n",
      "5526\n",
      "koger\n",
      "5522\n",
      "koger\n",
      "5523\n",
      "koger\n",
      "5521\n",
      "koger\n",
      "5513\n",
      "koger\n",
      "5501\n"
     ]
    }
   ],
   "source": [
    "tfidf_dir = pathlib.Path('data/4-papers-tfidf/')\n",
    "#print(tfs)\n",
    "for doc_id in tfs:\n",
    "    print(doc_id)\n",
    "    with open(tfidf_dir / f'{doc_id}.txt', 'w') as tfidf_file:\n",
    "        print(' '.join(map(str, compute_document_vector(word_vector, doc_id, tfs))), file=tfidf_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15093c7-0c0a-43f4-8270-937b16646cb7",
   "metadata": {},
   "source": [
    "We assemble all the TF-IDF values into a single file, one row per document.  It can be read into a numpy array for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "44da6cb8-a8be-4e86-a8b9-6b5044308237",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/4-papers-tfidf/*.txt > data/tf_idf.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "21ad40d7-f602-455e-b79e-436ecfd5fe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = np.genfromtxt('data/tf_idf.txt', delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dfb75ca2-238b-4800-852a-418cb8a2a6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4688)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5f095916-bb98-4451-b49f-466276892b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.min(), tf_idf.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e90d16-3921-4cf4-87f5-24f7d684743d",
   "metadata": {},
   "source": [
    "The matrix is indeed sparse, less than 2 % of the values is non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6530f472-9e90-4faa-bf1b-93750511a589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(tf_idf)/tf_idf.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4d9dfc-b0a3-4059-bf45-f43474977b1f",
   "metadata": {},
   "source": [
    "An interesting question is how many words out of our word vector are actually noninformative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "08425d81-2186-49b4-b2e8-f92d10fa1d62",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf_idf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m summary_vector \u001b[39m=\u001b[39m tf_idf\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf_idf' is not defined"
     ]
    }
   ],
   "source": [
    "summary_vector = tf_idf.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dbc98d-f033-43a5-aa15-498710cabd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66668"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summary_vector) - np.count_nonzero(summary_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1887ad-f7e5-415e-9ddf-7960b3a92549",
   "metadata": {},
   "source": [
    "Apparently, stop word removal was quite effective since only 39 words occur in all documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893a7a21-e4b5-4fbe-99ad-dd4d6fcfc156",
   "metadata": {},
   "source": [
    "## Salient words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfff1c8-323f-4527-818b-c8ecccb94381",
   "metadata": {},
   "source": [
    "Visualizing the distribution of TF-IDF values gives a hint at a proper thresholding value to reduce the number of components of the word vector to something more manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63af3b8-de54-4d50-8f1f-b0b16ed96d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(summary_vector, bins=np.logspace(-7, 0, 8), alpha=0.7)\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd410b2-e561-4cbc-a239-81029c5d0aeb",
   "metadata": {},
   "source": [
    "We set the cut-off for the TF-IDF values at $10^{-2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73acb14-f336-4b0e-8068-1da8f455dfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_cutoff = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab7d719-7ebb-4324-bd63-96cab6516fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_filtered = np.where(tf_idf > tf_idf_cutoff, tf_idf, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64583e4-2306-4864-8b58-ea95ce5929ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_filtered_summary = tf_idf_filtered.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd35a503-330d-4b45-9dcc-f3e2937265a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(tf_idf_filtered_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b475f8-4c03-4217-8d81-d3ca77187f01",
   "metadata": {},
   "source": [
    "At this level, the number of components that are non-zero for at least one document is less than 1000.  We can easily obtain the indices of the salient words in the word vector, and reduce the word vector and the TF-IDF matrix based on those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de2a958-6e8e-495a-b23c-52760034d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "salient_indices = np.nonzero(tf_idf_filtered_summary)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cff85d1-beda-457a-b4bd-182ea1992e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_salient = tf_idf[:, salient_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6db0ec-351c-448f-b305-fc1ad78ba643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 36)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_salient.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a065a5-9f9b-4936-9f0e-87b7e8c6946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('data/tf_idf_salient.txt', tf_idf_salient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d288a7-4a7f-40a8-a70f-15e42fc0550d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 7989 is out of bounds for axis 0 with size 4688",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m salient_word_vector \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(word_vector)[salient_indices]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 7989 is out of bounds for axis 0 with size 4688"
     ]
    }
   ],
   "source": [
    "salient_word_vector = np.array(word_vector)[salient_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bd72a2-e625-4843-8476-7d390695ea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "salient_word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2174eb25-9187-4307-932b-9db2c7db9011",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/salient_word_vector.txt', 'w') as file:\n",
    "    print('\\n'.join(salient_word_vector), file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a7cf57-57d7-479f-a936-abb625822f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_salient = np.genfromtxt('data/tf_idf_salient.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077d024e-6223-4693-bcb9-cce4840ebe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_salient.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ec43d1-cb8c-4c41-8318-ab4d0e702d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "salient_word_vector = np.genfromtxt('data/salient_word_vector.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59497246-1f83-4c35-ba09-cb24b35435a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "salient_word_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4a80d7-afae-4bb5-95fa-dbbf11568a40",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94495c0-ee2b-4869-90c5-6f7cd4a7c0e8",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2ec914-2e65-4db3-bbd6-4f79bdac6ada",
   "metadata": {},
   "source": [
    "The output (questions, subquestions, pillars) for each of the documents is stored in a CSV document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cade8a79-7d73-4b95-82cb-e6c590b2d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('data/efsa_cs1.csv', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eed6a8-f742-433d-852a-e15864b9500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a74ba6c-feaf-4a81-9f2f-2bd797e49db3",
   "metadata": {},
   "source": [
    "A classification is available for some documents that are missing from the training data, so we make sure those are dropped from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f56e3e0-c8a4-4bd9-9225-c23d8c20c882",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_doc_ids = set(metadata.EFSAID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9769e2-0302-4c4d-9a80-ccb1b9867efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = set(np.genfromtxt('data/doc_ids.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1790e1-1efa-47bd-9ecd-1a2b8dca316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_doc_ids - doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7357ef2d-2813-46de-a6d9-3974fd493eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids - metadata_doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6937894f-44eb-4acb-a7f4-886997f7858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = metadata[~metadata.EFSAID.isin(metadata_doc_ids - doc_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ce02c7-732c-46b0-995b-dfc2b8b0a1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2216f495-ed0b-4cdf-9612-e38b57ef1549",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = metadata[['EFSAID', 'QuestionID', 'SubQuestionID', 'PillarID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacbe887-6d03-4570-9a66-785242fcbe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24d9f4b-ffc2-4c0d-8f37-53cc5b83f68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.to_csv('data/metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98615380-a1a0-4bc6-83c8-30fd8e48f2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('data/metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb45723a-9779-4939-be12-3783f6653ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4df573-e39a-431e-824e-a2026ca3fe92",
   "metadata": {},
   "source": [
    "The output data for the documents is in a pandas dataframe that contains only positive data.  It would be more convenient to have it in a numpy array that contains either 0 or 1 for each document, depending on whether the document addresses the question/subquestion/pillar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76fad4e-fdab-4c60-b8e9-6f8d9fed9028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output(data, col_name, item_id):\n",
    "    output_col_name = 'output'\n",
    "    output = data[['EFSAID', col_name]].drop_duplicates().query(f'{col_name} == {item_id}')\n",
    "    output.rename(columns={col_name: output_col_name}, inplace=True)\n",
    "    output.output = 1\n",
    "    neg_ids = data[~data.EFSAID.isin(output.EFSAID)].EFSAID.unique()\n",
    "    neg_output = pd.DataFrame({\n",
    "        'EFSAID': neg_ids,\n",
    "        output_col_name: [0]*len(neg_ids),\n",
    "    })\n",
    "    return pd.concat([output, neg_output], axis=0) \\\n",
    "             .sort_values(by=['EFSAID']) \\\n",
    "             .reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f534168-5c90-4077-bfc9-46c053bc0aba",
   "metadata": {},
   "source": [
    "The data has to be split into three parts:\n",
    "1. training data\n",
    "1. validation data\n",
    "1. test data\n",
    "\n",
    "The training data will be used to determine the decision forest, the validation data will be used to estimate overfitting, and the test data will not be used at all during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969acafa-eb4c-4108-82df-863403aff9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_indices(output, fraction):\n",
    "    pos_output_1 = output[output.output == 1].sample(frac=fraction)\n",
    "    neg_output_1 = output[output.output == 0].sample(frac=fraction)\n",
    "    output_1 = pd.concat([pos_output_1, neg_output_1], axis=0).sort_values(by=['EFSAID'])\n",
    "    output_2 = output[~output.EFSAID.isin(output_1.EFSAID)].sort_values(by=['EFSAID'])\n",
    "    return output_1.index.values, output_2.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c014ea95-b31b-4608-9027-2cf04c7f67c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(tf_idf, output_df, indices):\n",
    "    input = tf_idf[indices, :]\n",
    "    output = output_df.iloc[indices].output.to_numpy()\n",
    "    return input, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dd8d59-7eb9-40f0-ae66-529dbad62717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(input_data, output_data, value_name, value,\n",
    "                    test_frac=0.2, val_frac=0.2, verbose=False):\n",
    "    value_output = create_output(output_data, value_name, value)\n",
    "    test_indices, training_indices = split_indices(value_output, test_frac)\n",
    "    val_indices, train_indices = split_indices(value_output.iloc[training_indices],\n",
    "                                               val_frac)\n",
    "    test_input, test_output = create_dataset(input_data, value_output, test_indices)\n",
    "    np.savetxt(f'data/6-validations/{value_name}_{value}_target_output.txt',\n",
    "               test_output)\n",
    "    val_input, val_output = create_dataset(input_data, value_output, val_indices)\n",
    "    train_input, train_output = create_dataset(input_data, value_output,\n",
    "                                               train_indices)\n",
    "    with open('data/6-validations/indices.txt', 'w') as file:\n",
    "        for index in train_indices:\n",
    "            print(f'{index},training', file=file)\n",
    "        for index in val_indices:\n",
    "            print(f'{index},validation', file=file)\n",
    "        for index in test_indices:\n",
    "            print(f'{index},test', file=file)\n",
    "    if verbose:\n",
    "        print(f'training data:   {train_input.shape} -> {train_output.shape}', file=sys.stderr)\n",
    "        print(f'validation data: {val_input.shape} -> {val_output.shape}', file=sys.stderr)\n",
    "        print(f'test data:       {test_input.shape} -> {test_output.shape}', file=sys.stderr)\n",
    "    return (\n",
    "        xgb.DMatrix(train_input, label=train_output),\n",
    "        xgb.DMatrix(val_input, label=val_output),\n",
    "        xgb.DMatrix(test_input)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d9933f-6032-455e-9745-8dbbd6c2c92e",
   "metadata": {},
   "source": [
    "We construct the XGBoost datasets for the training, validation and test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d657fb1-f375-4a00-9e2f-11f862e2233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_train_data, q1_val_data, q1_test_data = create_datasets(tf_idf_salient, metadata,\n",
    "                                                           'QuestionID', 1,\n",
    "                                                           verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3735671-f0b9-4035-9d71-d175698b208c",
   "metadata": {},
   "source": [
    "480 documents will be used for training, 119 for validation and 149 as test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4210c62a-3c3e-4407-82fe-3a4079420563",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3941048-0544-4fab-be47-4dfbf7bdb2a1",
   "metadata": {},
   "source": [
    "The evaluation list contains the data sets that are evaluated at the end of each training round.  This is the training and validation data as a list of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6f6ba1-cf69-477c-9642-075bfd4cb39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evallist = [(q1_train_data, 'train'), (q1_val_data, 'eval')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bc982e-f3aa-4e4d-b5fd-70bcd4508c3c",
   "metadata": {},
   "source": [
    "A number of hyperparameters can be specified.\n",
    "* `max_depth`: increases model complexity, hence potential overfitting.\n",
    "* `eta`: determines step size, when decreased, increase number of steps.\n",
    "* `nthread': number of threads to use for the learning process.\n",
    "* `eval_metric`: metric used to evaluate training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe52d8-6975-44a5-bff0-a536d3c92804",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'max_depth': 2,\n",
    "    'eta': 0.1,\n",
    "    'objective': 'binary:logistic',\n",
    "    'nthread': 4,\n",
    "    'eval_metric': 'auc',\n",
    "}\n",
    "nr_steps = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b449e6-00ec-4943-aadf-bdc1937409f3",
   "metadata": {},
   "source": [
    "In order to keep track of the training and validation error, we can define a dictionary to hold that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094e5d36-d29b-4fde-b5ea-3bf9891fb790",
   "metadata": {},
   "outputs": [],
   "source": [
    "progress = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95966bd-75b6-46ae-a90f-9531f8ead02f",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b09690-d5ca-4d86-b14f-62ef3261a4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.train(param, q1_train_data, nr_steps, evals_result=progress, evals=evallist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2ac0c9-4a18-4f00-b335-54f25f07d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_progress(progress, file_name):\n",
    "    train_progress = progress['train']['auc']\n",
    "    eval_progress = progress['eval']['auc']\n",
    "    time_progress = range(1, len(train_progress) + 1)\n",
    "    progress_data = np.array([time_progress, train_progress, eval_progress]).transpose()\n",
    "    np.savetxt(file_name, progress_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9c3ace-3754-46e6-8d12-a82184032dc9",
   "metadata": {},
   "source": [
    "Now the trained model can be used to predict whether a document answers the question, and we can compare this to the output for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb5b3ea-d1c7-4d08-8044-9fcc9c9a13f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_pred_output = bst.predict(q1_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea1989c-fa46-4cc9-9871-7551771c9c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(q1_test_output - q1_pred_output.round(0))/len(q1_test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea61d1d-5b7b-4177-81f1-fec732cce9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bst.save_model('models/q1_model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f21319-4944-463a-b192-cffcbc69d02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.Booster()\n",
    "bst.load_model('models/q1_model.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db130c0-5190-4688-95eb-8787b476fc1a",
   "metadata": {},
   "source": [
    "The importance of features in the model is indicated by the F-scores.  These can be visualized easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5a7291-db4d-451e-a6a9-d1e28fc10fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(bst, max_num_features=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e858ea2-e50c-4a5b-ac9c-0918d21e81db",
   "metadata": {},
   "source": [
    "The hyperparameters for the XGBoost algorithm are defined in a dictionary, and these can perhaps be fine-tuned in a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150d8afa-9854-419c-ab44-c209f6269df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'max_depth': 2,\n",
    "    'eta': 0.1,\n",
    "    'objective': 'binary:logistic',\n",
    "    'nthread': 4,\n",
    "    'eval_metric': 'auc',\n",
    "    'verbosity': 0,\n",
    "}\n",
    "nr_steps = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee9fff-2788-4636-8f62-7a14215ad139",
   "metadata": {},
   "source": [
    "We need a function to do the training and testing for each of the models we want to create.  This function will\n",
    "* extract the relevant training data based on the given value name and value,\n",
    "* train a model and save it to a file,\n",
    "* save the training progress to a file,\n",
    "* evaluate the test data and store the results,\n",
    "* evaluate all the data and store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c84c7e-08d3-4dc4-8329-569785865325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(input_data, output_data, value_name, value, params, verbose=False):\n",
    "    train_data, val_data, test_data = create_datasets(input_data, output_data,\n",
    "                                                      value_name, value,\n",
    "                                                      verbose=verbose)\n",
    "    evallist = [(train_data, 'train'), (val_data, 'eval')]\n",
    "    progress = {}\n",
    "    bst = xgb.train(param, train_data, nr_steps, evals=evallist,\n",
    "                    evals_result=progress, verbose_eval=False)\n",
    "    bst.save_model(f'models/{value_name}_{value}_model.json')\n",
    "    save_progress(progress, f'data/6-validations/{value_name}_{value}_auc.txt')\n",
    "    test_output = bst.predict(test_data)\n",
    "    np.savetxt(f'data/6-validations/{value_name}_{value}_output.txt',\n",
    "               test_output)\n",
    "    all_data = xgb.DMatrix(input_data)\n",
    "    all_output = bst.predict(all_data)\n",
    "    np.savetxt(f'data/5-predictions/{value_name}_{value}_output.txt',\n",
    "               all_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3cc025-2bb9-42a3-8c2c-4c253b2b91af",
   "metadata": {},
   "source": [
    "Create the models and evaluation results for all questions, subquestions and pillars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2dad3f-5654-475c-869a-259de993f860",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question_id in metadata.QuestionID.unique():\n",
    "    train_eval(tf_idf_salient, metadata, 'QuestionID', question_id, param, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b4ec69-a438-4650-933c-867b4dad3dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pillar_id in metadata.PillarID.unique():\n",
    "    train_eval(tf_idf_salient, metadata, 'PillarID', pillar_id, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121a1300-4a53-4934-9588-84c26548d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subquestion_id in metadata.SubQuestionID.unique():\n",
    "    train_eval(tf_idf_salient, metadata, 'SubQuestionID', subquestion_id, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455a1c8a-ada1-4de0-939f-69e488e88df6",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86b5a1b-b803-4e25-b3a4-88a0fcbda4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(value_name, value):\n",
    "    output = np.genfromtxt(f'data/6-validations/{value_name}_{value}_output.txt')\n",
    "    target_output = np.genfromtxt(f'data/6-validations/{value_name}_{value}_target_output.txt')\n",
    "    return np.count_nonzero(target_output - output.round(0))/len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0521da1-c93e-4439-bbfa-31b26e1aa189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confustion_matrix(value_name, value):\n",
    "    output = np.genfromtxt(f'data/6-validations/{value_name}_{value}_output.txt')\n",
    "    target_output = np.genfromtxt(f'data/6-validations/{value_name}_{value}_target_output.txt')\n",
    "    df = pd.DataFrame({\n",
    "        'target': target_output,\n",
    "        'output': output,\n",
    "    })\n",
    "    true_pos_out = len(df[(df.target > 0.5) & (df.output > 0.5)])\n",
    "    false_pos_out = len(df[(df.target <= 0.5) & (df.output > 0.5)])\n",
    "    true_neg_out = len(df[(df.target <= 0.5) & (df.output <= 0.5)])\n",
    "    false_neg_out = len(df[(df.target > 0.5) & (df.output <= 0.5)])\n",
    "    return [\n",
    "        [true_pos_out, false_pos_out],\n",
    "        [false_neg_out, true_neg_out],\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c892ece4-8d98-49de-a877-a33f17c3c019",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question_id in range(1, 11):\n",
    "    print(confustion_matrix('QuestionID', question_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e413c6f9-2ea7-4e14-9699-28529491c29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pillar_id in range(1, 5):\n",
    "    print(confustion_matrix('PillarID', pillar_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040d1795-72fc-4c30-9bfe-ad105e4c6900",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subquestion_id in range(1, 400):\n",
    "    try:\n",
    "        print(f'{subquestion_id:3d}',\n",
    "              f'{str(confustion_matrix(\"SubQuestionID\", subquestion_id)):25s}',\n",
    "              f'{evaluate(\"SubQuestionID\", subquestion_id):6.3f}')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bb41c9-a96b-464f-9725-dadd4fb549b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_score(file_name):\n",
    "    data = np.genfromtxt(file_name, delimiter=',', names=True)\n",
    "    name = data.dtype.names[0]\n",
    "    plt.bar(data[name], data['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716080df-bc35-4e75-be8c-cf485cee68f3",
   "metadata": {},
   "source": [
    "Write the test scores to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cecb3da-d37c-41a7-8bc4-1e4267bdf6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/6-validations/test_questions_score.txt', 'w') as file:\n",
    "    print('question_id,score', file=file)\n",
    "    for question_id in sorted(metadata.QuestionID.unique()):\n",
    "        print(f\"{question_id:3d},{evaluate('QuestionID', question_id):.3f}\", file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1f6526-d117-4705-af73-40c14d64b893",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score('data/6-validations/test_questions_score.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baea9e7-4550-4d63-9758-543d29f29cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/6-validations/test_pillars_score.txt', 'w') as file:\n",
    "    print('pillar_id,score', file=file)\n",
    "    for pillar_id in sorted(metadata.PillarID.unique()):\n",
    "        print(f\"{pillar_id:3d},{evaluate('PillarID', pillar_id):.3f}\", file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef1b63a-ec4d-42de-9937-3089f63fef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score('data/6-validations/test_pillars_score.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea72b60-07ec-46cc-8535-f5929e21b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/6-validations/test_subquestions_score.txt', 'w') as file:\n",
    "    print('subquestion_id,score', file=file)\n",
    "    for subquestion_id in sorted(metadata.SubQuestionID.unique()):\n",
    "        print(f\"{subquestion_id:3d},{evaluate('SubQuestionID', subquestion_id):.3f}\", file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837e4d46-409c-4467-a1f3-9d52d9b02e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score('data/6-validations/test_subquestions_score.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e242aeb8-2146-4f44-ac23-280df2c4d702",
   "metadata": {},
   "source": [
    "To visualize the training progress, we need a function to plot the AUC for the training and the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649030bb-deb6-4b24-99d7-483224488ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_progress(value_name, value):\n",
    "    progress_data = np.genfromtxt(f'data/6-validations/{value_name}_{value}_auc.txt')\n",
    "    plt.plot(progress_data[:, 0], progress_data[:, 1], label='training')\n",
    "    plt.plot(progress_data[:, 0], progress_data[:, 2], label='evaluation')\n",
    "    plt.xlabel('nr. steps')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8542164-6893-4ea7-8e72-c10ffc85bda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_progress('QuestionID', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdad961-4498-49dd-b0ef-fb0a6f5568cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_indices(file_name):\n",
    "    indices = pd.read_csv(file_name, names=['doc_index', 'set_name'])\n",
    "    return (\n",
    "        indices[indices.set_name == 'training'].doc_index.tolist(),\n",
    "        indices[indices.set_name == 'validation'].doc_index.tolist(),\n",
    "        indices[indices.set_name == 'test'].doc_index.tolist(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72753369-6e23-4116-b633-e0c09d24cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, val_indices, test_indices = read_indices('data/6-validations/indices.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf113af-09a8-43fa-9d68-d3a2aee85d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target(metadata):\n",
    "    targets = []\n",
    "    for doc in metadata.groupby(['EFSAID']):\n",
    "        data = {\n",
    "            'efsa_id': doc[0],\n",
    "            'questions': set(doc[1].QuestionID),\n",
    "            'pillars': set(doc[1].PillarID),\n",
    "            'subquestions': set(doc[1].SubQuestionID),\n",
    "        }\n",
    "        targets.append(data)\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c91c5d-5960-468b-b044-45f755ce3762",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = create_target(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ba64e-8e0a-4b08-bd78-f383d10e17bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d54bb0-f86f-4977-8eb6-d1268e070603",
   "metadata": {},
   "source": [
    "# Hierarchical models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117bc07c-9f6e-4361-a6f3-b0ef05ac699b",
   "metadata": {},
   "source": [
    "We know from the analysis of the data that for any two questions, the associated subquestions form disjoint sets, and similar for the subquestions associated with pillars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3cf9aa-a7bd-4796-81d9-42197a170e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_subquestions = {group[0]: set(group[1].SubQuestionID)\n",
    "                         for group in metadata[['QuestionID', 'SubQuestionID']].drop_duplicates().groupby('QuestionID')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dce1a9-1f6f-4403-b630-1f150b28c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(question_subquestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8ea214-94fe-4e42-83f1-f5e2510408c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pillar_subquestions = {group[0]: set(group[1].SubQuestionID)\n",
    "                         for group in metadata[['PillarID', 'SubQuestionID']].drop_duplicates().groupby('PillarID')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976ec63c-0709-4e83-986c-309fbab72139",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pillar_subquestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93efb443-8198-426b-a4ff-9943ba28584b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "efsa_cs1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "365a0841636329a8a331746f37b511b0c03e6141f7d656688e158bc3ee832d6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
